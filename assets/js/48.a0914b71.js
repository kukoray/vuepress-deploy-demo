(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{469:function(t,a,s){"use strict";s.r(a);var r=s(65),e=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"感知机"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#感知机"}},[t._v("#")]),t._v(" 感知机")]),t._v(" "),s("h2",{attrs:{id:"感知机-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#感知机-2"}},[t._v("#")]),t._v(" 感知机")]),t._v(" "),s("p",[t._v("感知机（perception）")]),t._v(" "),s("p",[t._v("感知机是一个"),s("strong",[t._v("二分类")]),t._v("的"),s("strong",[t._v("线性")]),t._v("分类器，属于"),s("strong",[t._v("判别模型")]),t._v("；要求训练的数据是线性可分，即可以用一个超平面将数据集分为隔开。")]),t._v(" "),s("p",[t._v("模拟生物学上的神经元，树突、轴突、细胞核那种触发方式")]),t._v(" "),s("p",[t._v("有 input ；weight ； 激活函数 ； output")]),t._v(" "),s("p",[t._v("存在的问题：感知机无法解决异或问题！")]),t._v(" "),s("p",[t._v("原因：感知机在二维平面中是一条直线，只能解决线性二分类问题")]),t._v(" "),s("p",[t._v("范数的概念：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://s2.loli.net/2022/09/13/Iw3g2WFY68HKDk4.png",alt:"preview"}})]),t._v(" "),s("p",[t._v("这里介绍感知机的损失函数，这里用"),s("strong",[t._v("点到平面的距离")]),t._v("来作为他的损失函数，公式如下：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://s2.loli.net/2022/09/13/e1aXb3ofW2NLTls.png",alt:"image-20220913151423872"}})]),t._v(" "),s("h2",{attrs:{id:"多层感知机"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多层感知机"}},[t._v("#")]),t._v(" 多层感知机")]),t._v(" "),s("p",[t._v("Multi Layer Perception")]),t._v(" "),s("p",[t._v("引入了更多的隐藏层（每个隐藏层都要有激活函数）")]),t._v(" "),s("p",[t._v("激活函数的重要性和作用：")]),t._v(" "),s("ol",[s("li",[t._v("没有激活函数（或者 激活函数为 线性激活函数），MLP等价于单层网络，会引起网络的退化")]),t._v(" "),s("li",[t._v("激活函数可以让多层感知机变成真正的多层，而不是退化为1层")]),t._v(" "),s("li",[t._v("引入非线性，使得网格可以无限逼近与任意非线性函数")])]),t._v(" "),s("p",[t._v("激活函数需要具备以下几点性质:")]),t._v(" "),s("ol",[s("li",[t._v("连续并可导（"),s("strong",[t._v("允许少数点上不可导")]),t._v("relu)，便于利用数值优化的方法来学习网络参数")]),t._v(" "),s("li",[t._v("激活函数及其导函数要尽可能的简单，有利于提高网络计算效率")]),t._v(" "),s("li",[t._v("激活函数的导函数的值域要在合适区间内，不能太大也不能太小，否则会影响训练的效率和稳定性")])]),t._v(" "),s("p",[t._v("sigmoid ：通常作为二分类问题的激活函数；也用于神经网络中门控单元的作用，用于决定神经元的输出是遗忘还是保留")]),t._v(" "),s("p",[t._v("tanh：双曲正切，特点：数值的对称性，0均值；值域-1 到1；线性区域；存在饱和区（梯度为0，不容易训练）；")]),t._v(" "),s("p",[t._v("relu：最常用！！！非饱和")]),t._v(" "),s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://s2.loli.net/2022/05/01/IVpJzcNdMeLFkqi.png",alt:"image-20220501161503043"}}),t._v(" "),s("p",[s("img",{attrs:{src:"https://www.zhihu.com/equation?tex=Softmax(z_%7Bi%7D)%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D",alt:"[公式]"}})]),t._v(" "),s("h2",{attrs:{id:"反向传播"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#反向传播"}},[t._v("#")]),t._v(" 反向传播")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://s2.loli.net/2022/05/01/r3DMvFSLIN4qhVT.png",alt:"image-20220501162825470"}})]),t._v(" "),s("p",[t._v("​\t\t\t计算i-1层的误差函数，可以使用第i层以求的误差函数值 * 这一层的对上一层的偏导数。")]),t._v(" "),s("p",[t._v("其中w权值有多少个连接该节点，就会有多少个相加在一起")]),t._v(" "),s("h2",{attrs:{id:"损失函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#损失函数"}},[t._v("#")]),t._v(" 损失函数")]),t._v(" "),s("p",[t._v("损失函数Loss、代价函数Cost、目标函数Objective")]),t._v(" "),s("p",[t._v("损失函数：单样本的（可以理解为一个样本，在实际时是一个batch）")]),t._v(" "),s("p",[t._v("代价函数：总体损失函数的平均值（1/n *sum（loss））")]),t._v(" "),s("p",[t._v("obj =  cost + regularization")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/01/gdu2v7ZBC6LmSiF.png",alt:"image-20220501171345374"}}),t._v(" "),s("h3",{attrs:{id:"均方误差mse"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#均方误差mse"}},[t._v("#")]),t._v(" 均方误差MSE")]),t._v(" "),s("p",[t._v("常用于回归任务中")]),t._v(" "),s("h3",{attrs:{id:"交叉熵ce"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#交叉熵ce"}},[t._v("#")]),t._v(" 交叉熵CE")]),t._v(" "),s("p",[s("strong",[t._v("信息熵")])]),t._v(" "),s("p",[t._v("定义：描述信息的不确定度")]),t._v(" "),s("p",[t._v("越不确定，其信息熵越大！  越确定的事件，其信息熵越小。")]),t._v(" "),s("p",[t._v("一件100%确定发生的事情，其信息熵为0；但0%确定发生的事情，其信息熵也为0；")]),t._v(" "),s("p",[t._v("但是50%发生的事情，其信息熵就较大（但不一定是50%是最大，这取决于p log（p） 中对数的真数值）")]),t._v(" "),s("img",{staticStyle:{zoom:"100%"},attrs:{src:"https://bkimg.cdn.bcebos.com/formula/38966b06d2b296a5c70e36df8f1e4027.svg",alt:"img"}}),t._v(" "),s("p",[s("strong",[t._v("玻尔兹曼公式")])]),t._v(" "),s("img",{staticStyle:{zoom:"100%"},attrs:{src:"https://bkimg.cdn.bcebos.com/formula/083633bd66aa6ef2295ea1c9c27c6077.svg",alt:"img"}}),t._v(" "),s("p",[t._v("可以明显看出“信息熵”的定义和“热力学熵”（玻尔兹曼公式）的定义只相差某个比例常数。")]),t._v(" "),s("p",[s("strong",[t._v("交叉熵")]),t._v("（Cross Entropy）：衡量两个概率分布的差异")]),t._v(" "),s("p",[t._v("交叉熵的好伙伴（一定是成对出现的）：SoftMax函数，将数据变换到符合概率分布的形式")]),t._v(" "),s("img",{staticStyle:{zoom:"40%"},attrs:{src:"https://s2.loli.net/2022/05/01/iwRe8MNFCxuVQtA.png",alt:"image-20220501184312481"}}),t._v(" "),s("img",{staticStyle:{zoom:"47%"},attrs:{src:"https://s2.loli.net/2022/05/01/kzed4jqTR17PQp8.png",alt:"image-20220501185604249"}}),t._v(" "),s("p",[t._v("例子：")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/01/qPhul3O1HUrkV5X.png",alt:"image-20220501193126447"}}),t._v(" "),s("p",[t._v("SoftMax")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/01/uylIPNpMdFeUmKc.png",alt:"image-20220501193521221"}}),t._v(" "),s("h2",{attrs:{id:"权值初始化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#权值初始化"}},[t._v("#")]),t._v(" 权值初始化")]),t._v(" "),s("h3",{attrs:{id:"随机初始化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#随机初始化"}},[t._v("#")]),t._v(" 随机初始化")]),t._v(" "),s("p",[t._v("若"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/828980",target:"_blank",rel:"noopener noreferrer"}},[t._v("随机变量"),s("OutboundLink")],1),t._v("X服从一个"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B/5362790",target:"_blank",rel:"noopener noreferrer"}},[t._v("数学期望"),s("OutboundLink")],1),t._v("为μ、"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%96%B9%E5%B7%AE/3108412",target:"_blank",rel:"noopener noreferrer"}},[t._v("方差"),s("OutboundLink")],1),t._v("为σ2的正态分布，记为N(μ，σ2)。其"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0/5021996",target:"_blank",rel:"noopener noreferrer"}},[t._v("概率密度函数"),s("OutboundLink")],1),t._v("为正态分布的"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%9C%9F%E6%9C%9B%E5%80%BC/8664642",target:"_blank",rel:"noopener noreferrer"}},[t._v("期望值"),s("OutboundLink")],1),t._v("μ决定了其位置，其"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E5%B7%AE/1415772",target:"_blank",rel:"noopener noreferrer"}},[t._v("标准差"),s("OutboundLink")],1),t._v("σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是"),s("a",{attrs:{href:"https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83",target:"_blank",rel:"noopener noreferrer"}},[t._v("标准正态分布"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("img",{staticStyle:{zoom:"99%"},attrs:{src:"https://bkimg.cdn.bcebos.com/formula/01a91c9fd0fb903c925ffe1faa8bcbfe.svg",alt:"img"}}),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/01/pxA7LoiqsEjZY3C.jpg",alt:"图1 标准正态分布"}}),t._v(" "),s("p",[t._v("3σ准则：（μ-3σ，μ+3σ）中的概率为99.73%")]),t._v(" "),s("p",[t._v("高斯分布（正态分布）随机初始化")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://s2.loli.net/2022/05/01/xdyGkhPW1z8Ngao.png",alt:"image-20220501200207575"}})]),t._v(" "),s("h2",{attrs:{id:"正则化方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#正则化方法"}},[t._v("#")]),t._v(" 正则化方法")]),t._v(" "),s("p",[t._v("regularization：减小方差的策略，通俗理解就是减轻过拟合的策略")]),t._v(" "),s("p",[t._v("误差=偏差+方差+噪声")]),t._v(" "),s("p",[t._v("偏差：算法本身的拟合能力")]),t._v(" "),s("p",[t._v("方差：算法的迁移能力、泛化能力")]),t._v(" "),s("p",[t._v("噪声：期望泛化误差下界（即 ：即使做到最好，也仍然存在的误差）")]),t._v(" "),s("img",{staticStyle:{zoom:"33%"},attrs:{src:"https://s2.loli.net/2022/05/01/6lDiUcoqFEPg3JY.png",alt:"image-20220501205837040"}}),t._v(" "),s("p",[t._v("对于正则化，我们常用")]),t._v(" "),s("h3",{attrs:{id:"l1-regularization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#l1-regularization"}},[t._v("#")]),t._v(" L1 regularization")]),t._v("\nL(\\mathrm{w})=E_{D}(w)+\\frac{\\lambda}{n} \\sum_{i=1}^{n}\\left|w_{t}\\right|\\\\\n\nL(\\mathrm{w})=E_{D}(w)+\\frac{\\lambda}{n} \\sum_{i=1}^{n}\\left|w_{i}\\right|\\\\\n\n\\frac{\\partial L(w)}{\\partial w}=\\frac{\\partial E_{D}(w)}{\\partial w}+\\frac{\\lambda \\operatorname{sgn}(w)}{n}\\\\\n\nw^{\\prime}=w-\\eta \\frac{\\partial L(w)}{\\partial w} \\\\\nw^{\\prime}=w-\\frac{\\eta \\lambda \\operatorname{sgn}(w)}{n}-\\frac{\\eta\\partial E_{D}(w)}{\\partial w}\n\n"),s("p",[t._v("上式可知，当w大于0时，更新的参数w变小；当w小于0时，更新的参数w变大"),s("strong",[t._v("L1正则化容易使参数变为0，即特征稀疏化。")])]),t._v(" "),s("h3",{attrs:{id:"l2-regularization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#l2-regularization"}},[t._v("#")]),t._v(" L2 regularization")]),t._v(" "),s("p",[t._v("作用：weight decay（权值衰减）")]),t._v(" "),s("p",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("mi",[t._v("λ")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\lambda")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.69444em","vertical-align":"0em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{staticClass:"mord mathit"},[t._v("λ")])])])]),t._v("是权衡Loss和正则项 的 一个比例系数，一般在（0-1），权衡于优化过程中更关注于loss 还是 正则项。")]),t._v("\nL(w)=E_{D}(w)+\\frac{\\lambda}{2 n} \\sum_{i=1}^{n} w_{\\mathrm{i}}^{2}\\\\\\\\\n\n求  L(\\mathrm{w})  的梯度 :\\\\\n\n\\frac{\\partial L(w)}{\\partial w}=\\frac{\\partial E_{D}(w)}{\\partial w}+\\lambda w\\\\\\\\\n\n参数 w更新:\\\\\n\nw^{\\prime}=w-\\eta \\frac{\\partial L(w)}{\\partial w} \\\\\nw^{\\prime}=w-{\\eta \\lambda w}-\\frac{\\eta \\partial E_{D}(w)}{\\partial w} \\\\\nw^{\\prime}=\\left(1-{\\eta \\lambda}\\right) w-\\frac{\\eta \\partial E_{D}(w)}{\\partial w}\n\n"),s("p",[t._v("由上式可知，正则化的更新参数相比于未含正则项的更新参数多了"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("mrow",[s("mi",[t._v("η")]),s("mi",[t._v("λ")])],1),s("mi",[t._v("w")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\eta \\lambda}w")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{staticClass:"mord textstyle uncramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("η")]),s("span",{staticClass:"mord mathit"},[t._v("λ")])]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")])])])]),t._v("，所以当"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("mi",[t._v("w")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("w")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")])])])]),t._v("趋向于0时，参数减小的非常缓慢，因此L2正则化使参数减小到很小的范围，但不为0。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://s2.loli.net/2022/05/01/ONqhf4awBo8sce3.png",alt:"image-20220501214930477"}})]),t._v(" "),s("h3",{attrs:{id:"dropout"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dropout"}},[t._v("#")]),t._v(" Dropout")]),t._v(" "),s("p",[t._v("dropout：随机失活")]),t._v(" "),s("p",[t._v("优点：避免网络训练过程中，过度依赖于某个神经元，从而实现减轻过拟合")]),t._v(" "),s("p",[t._v("随机：dropout probability（eg：p=0.5）")]),t._v(" "),s("p",[t._v("失活：weight = 0")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/01/4lwc9CxXvsGWh2T.png",alt:"image-20220501215716568"}}),t._v(" "),s("h3",{attrs:{id:"batch-normalization-bn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#batch-normalization-bn"}},[t._v("#")]),t._v(" Batch normalization（BN）")]),t._v(" "),s("p",[t._v("BN又叫批量标准化，"),s("strong",[t._v("BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。")])]),t._v(" "),s("p",[t._v("有之前的工作说明对图像的像素值分布变换为以0为均值，单位方差的正态分布数值时(这种操作被称为whiten)，可以加速网络收敛。现在换作深度神经网络每一隐藏层的输入也可以做whiten吧？这样BN出现了。")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://www.cnblogs.com/guoyaohua/p/8724433.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("【深入BN底层原理】深入理解Batch Normalization批标准化 - 郭耀华 - 博客园 (cnblogs.com)"),s("OutboundLink")],1)]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("BN原因")]),t._v(" "),s("p",[t._v("随着网络的深度增加，每层特征值分布会逐渐的向激活函数的输出区间的上下两端（激活函数饱和区间）靠近，这样继续下去就会导致梯度消失。"),s("strong",[t._v("BN就是通过方法将该层特征值分布重新拉回标准正态分布")]),t._v("，特征值将落在激活函数对于输入较为敏感的区间，输入的小变化可导致损失函数较大的变化，使得梯度变大，避免梯度消失，同时也可加快收敛。")])]),t._v(" "),s("h3",{attrs:{id:"layer-normalization-ln"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#layer-normalization-ln"}},[t._v("#")]),t._v(" Layer normalization（LN）")]),t._v(" "),s("h3",{attrs:{id:"concept-whitening"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#concept-whitening"}},[t._v("#")]),t._v(" Concept Whitening")]),t._v(" "),s("p",[t._v("概念白化")])])}),[],!1,null,null,null);a.default=e.exports}}]);