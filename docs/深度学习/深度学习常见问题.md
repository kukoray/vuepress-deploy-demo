# 常见问题



## BP反向传播







## 梯度消失-爆炸

​		目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

​		而链式法则是一个连乘的形式，**所以当层数越深的时候，梯度将以指数形式传播**。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。


### 梯度消失

​		经常出现，产生的原因有：一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。

​		当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致**靠近输入层的隐藏层权值更新缓慢或者更新停滞**。这就导致在训练时，只等价于后面几层的浅层网络的学习。

梯度消失的影响：

1. 浅层基本不学习，后面几层一直在学习，失去深度的意义。

2. 无法收敛，相当于浅层网络。



### 梯度爆炸

​		根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。这种情况又会导致靠近输入层的隐含层神经元调整变动极大。

​		梯度爆炸一般出现在深层网络和权值初始化值太大的情况下。另外，初始学习率太小或太大也会出现梯度消失或爆炸。

梯度爆炸的影响：

1. 模型不稳定，导致更新过程中的损失出现显著变化；
2. 训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。





## 梯度下降法



首先梯度下降法不是下降最快的方向；



概念理解有误，梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。

为什么在一般问题里梯度下降比牛顿类算法更常用呢？因为对于规模比较大的问题，Hessian计算是非常耗时的；同时对于很多对精度需求不那么高的问题，梯度下降的收敛速度已经足够了。而这也motivate了一类quasi-Newton算法，可以在规避Hessian计算的前提下达到于牛顿类算法差不多的收敛速度。

非线性规划当前的一个难点在于处理非凸问题的全局解，而搜索全局解这个问题一般的梯度下降也无能为力。

<img src="https://s2.loli.net/2022/04/27/MaOQrct5ipyTLdK.png" alt="image-20220427140458913" style="zoom:50%;" />



![image-20220427141416899](C:/Users/Jacky/AppData/Roaming/Typora/typora-user-images/image-20220427141416899.png)



梯度下降法结束条件：

1. （代价函数）损失函数小于一定的值e；！！！最常用
2. n到达最大迭代次数N；
3. 梯度值小于一定的值e；



## 如何解决过拟合和欠拟合？

一、造成过拟合的原因和解决方法：

模型复杂度过高，训练数据少，训练误差小，但是测试误差大

解决方法：

•	(1)从数据入手，获得更多的训练数据。使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减少噪音的影响，当然，直接增加实验数据一般是很困难的，但是可以通过一定的规则来扩充训练数据。比如，在图像分类的问题上，可以通过图像的平移、旋转、缩放等方式扩充数据；更进一步地，可以使用生成式对抗网络来合成大量的新训练数据

•	(2)降低模型复杂度。在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免拟合过多的采样噪音。例如，在神经网络中减少网络层数、神经元个数等;在决策树模型中降低树的深度、进行剪枝等

•	(3)正则化方法。在模型算法中添加惩罚函数来防止过拟合。

•	(4)集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险

二、造成欠拟合的原因和解决方法：

模型复杂度过低，无法很好的去拟合所有的训练数据，导致训练误差大

解决方法:

•	(1)添加新特征。当特征不足或现有特征与样本标签的相关性不强时，模型容易出现不拟合，通过挖掘'上下文特征''ID类特征''组合特征'等新的特征，往往能够取得更好的效果，在深度学习的潮流中，有很多类型可以帮组完成特征工程，如因子分解机

•	(2)增加模型复杂度。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力,例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数等

•	(3)减少正则化系数。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要针对性地减少正则化系数



## 如何解决梯度消失和梯度爆炸的问题？

梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。解决梯度消失、爆炸主要有以下几种方法：

（1） pre-training+fine-tunning

基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优。

（2） 梯度剪切：对梯度设定阈值

梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

（3） 权重正则化

正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。比较常见的是 L1 正则和 L2 正则，在各个深度框架中都有相应的API可以使用正则化。

（4） 选择relu等梯度大部分落在常数上的激活函数

relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

（5） batch normalization

BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

（6） 残差网络的捷径（shortcut）

（7） LSTM的“门（gate）”结构

LSTM 通过它内部的“门”可以在接下来更新的时候“记住”前几次训练的“残留记忆”。



## 梯度下降算法有哪些，描述下？

批量梯度下降法（Batch Gradient Descent）

容易得到最优解，但是由于每次考虑所有样本，速度很慢

随机梯度下降法（Stochastic Gradient Descent）

每次找一个样本，迭代速度快，但不一定每次都朝着收敛的方向

小批量梯度下降法（Mini-batch Gradient Descent）

（中庸的方法）每次更新选择一小部分数据来算，很实用





##  **P-R曲线**

​    即Precision-Recall曲线

​    P=TP/(TP+FP) ，P是代表预测为真且真实为真的数据占预测为真数据的比例。

​    R=TP/(TP+FN)，R是代表预测为真且真实为真的数据占真实为真数据的比例





## ROC曲线

中文名：受试者工作特征曲线

适用场景：二值分类问题，最重要的指标对于二值分类

二值分类的评价指标：precision，recall，F1，score P-R，ROC



ROC曲线的横坐标为假阳性率FPR，纵坐标为真阳性率TPR。

FPR＝FP／Ｎ　　　　　　TPR＝TP／Ｐ

![在这里插入图片描述](https://s2.loli.net/2022/05/18/nDwulUXCVriWgb4.png)

​		Ｐ是真实的正样本数量，Ｎ是真实的负样本数量，ＴＰ是Ｐ个正样本中被分类器预测为正样本的个数，ＦＰ是Ｎ个负样本中被分类器预测为正样本的个数。

> 例子：假如有１０位疑似癌症患者，其３人很不幸确实是患了Ｐ＝３，另外７人不是Ｎ＝７　。医院对１０位做出了诊断，针对３位癌症患者，其中２位确实是患者ＴＰ＝２，。那么真阳性率ＴＰ／Ｐ＝２／３。对于７位不是患者来说有一个误诊为那么ＦＰ＝１，那么假阳性率是ＦＰ／Ｎ＝１／７。对于该医院来说，这组分类结果就对应ＲＯＣ曲线上一个点（１／７，２／３）。

​		ＡＵＣ指的是ＲＯＣ曲线的面积大小，该值能够量化地反应基于ＲＯＣ曲线衡量出的模型性能。计算ＡＵＣ值只需要对曲线做积分既可。由于ＲＯＸ一般都处于ｙ＝ｘ这条直线的上方，所以ＡＵＣ的取值一般在０.５－１之间。**ＡＵＣ越大说明分类效果越好。**



相对于Ｐ－Ｒ曲线，ＲＯＣ曲线有一个特点，当正负样本的分布发生变化时，ＲＯＣ曲线的形状能够基本保持不变，而Ｐ－Ｒ曲线的形状一般会发生激烈的变化。



