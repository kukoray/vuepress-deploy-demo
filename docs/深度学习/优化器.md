# 优化器



机器学习中，通常有很多方法来试图寻找模型的最优解。比如常见的

**梯度下降法(Gradient Descent)**

> 随机梯度下降法SGD
>
> 小批量梯度下降MSGD
>
> 批量梯度下降法BGD

**动量优化法(Momentum)**



**自适应学习率优化算法**

> AdaGrad算法
> RMSProp算法
> Adam算法（吸收了adaGrad和RMSProp的优点）
> lazyadam算法





## SGD随机梯度下降

由于梯度下降，每次迭代需要用到所有的数据，耗时很长，计算量很大。


