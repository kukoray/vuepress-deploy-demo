<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Segment Anything 论文解读 | JACKY</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/logo.png">
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.e4d8b86b.css" as="style"><link rel="preload" href="/assets/js/app.6d855320.js" as="script"><link rel="preload" href="/assets/js/2.d4ff5c60.js" as="script"><link rel="preload" href="/assets/js/79.86f9f4d3.js" as="script"><link rel="prefetch" href="/assets/js/10.0677b883.js"><link rel="prefetch" href="/assets/js/100.47224151.js"><link rel="prefetch" href="/assets/js/101.e068f649.js"><link rel="prefetch" href="/assets/js/102.2216aa0a.js"><link rel="prefetch" href="/assets/js/103.1274fdb9.js"><link rel="prefetch" href="/assets/js/104.3894d45f.js"><link rel="prefetch" href="/assets/js/105.c45ae0dd.js"><link rel="prefetch" href="/assets/js/106.1ea76166.js"><link rel="prefetch" href="/assets/js/107.e9e69cdf.js"><link rel="prefetch" href="/assets/js/108.86dc789c.js"><link rel="prefetch" href="/assets/js/109.dff0ff70.js"><link rel="prefetch" href="/assets/js/11.0ce9576d.js"><link rel="prefetch" href="/assets/js/110.3a698495.js"><link rel="prefetch" href="/assets/js/111.8fc0aa53.js"><link rel="prefetch" href="/assets/js/112.aebd4f61.js"><link rel="prefetch" href="/assets/js/113.ac824a62.js"><link rel="prefetch" href="/assets/js/114.09b2f358.js"><link rel="prefetch" href="/assets/js/115.31a681fc.js"><link rel="prefetch" href="/assets/js/116.2eb333c2.js"><link rel="prefetch" href="/assets/js/117.45d3427d.js"><link rel="prefetch" href="/assets/js/118.313f7765.js"><link rel="prefetch" href="/assets/js/119.985d374f.js"><link rel="prefetch" href="/assets/js/12.809e25a2.js"><link rel="prefetch" href="/assets/js/120.41447dc4.js"><link rel="prefetch" href="/assets/js/121.f014baf8.js"><link rel="prefetch" href="/assets/js/122.097f6079.js"><link rel="prefetch" href="/assets/js/123.afc6a702.js"><link rel="prefetch" href="/assets/js/124.01d724f7.js"><link rel="prefetch" href="/assets/js/125.c9dc4beb.js"><link rel="prefetch" href="/assets/js/126.d5b795cb.js"><link rel="prefetch" href="/assets/js/127.ca74f94d.js"><link rel="prefetch" href="/assets/js/128.d2da9329.js"><link rel="prefetch" href="/assets/js/129.c8717ca5.js"><link rel="prefetch" href="/assets/js/13.152c7e44.js"><link rel="prefetch" href="/assets/js/14.1e51776e.js"><link rel="prefetch" href="/assets/js/15.baa617fb.js"><link rel="prefetch" href="/assets/js/16.f85fffe6.js"><link rel="prefetch" href="/assets/js/17.e6803744.js"><link rel="prefetch" href="/assets/js/18.6d74ca46.js"><link rel="prefetch" href="/assets/js/19.ab7532ab.js"><link rel="prefetch" href="/assets/js/20.be01556b.js"><link rel="prefetch" href="/assets/js/21.d9aded00.js"><link rel="prefetch" href="/assets/js/22.59fd96e2.js"><link rel="prefetch" href="/assets/js/23.575b0571.js"><link rel="prefetch" href="/assets/js/24.d7911b48.js"><link rel="prefetch" href="/assets/js/25.ee1ddc47.js"><link rel="prefetch" href="/assets/js/26.2645d0d2.js"><link rel="prefetch" href="/assets/js/27.b445999f.js"><link rel="prefetch" href="/assets/js/28.e93df91c.js"><link rel="prefetch" href="/assets/js/29.2e9e94e3.js"><link rel="prefetch" href="/assets/js/3.cce6fa70.js"><link rel="prefetch" href="/assets/js/30.c46c1576.js"><link rel="prefetch" href="/assets/js/31.d700821d.js"><link rel="prefetch" href="/assets/js/32.4017f172.js"><link rel="prefetch" href="/assets/js/33.c2f92eee.js"><link rel="prefetch" href="/assets/js/34.384d918e.js"><link rel="prefetch" href="/assets/js/35.9c122bb0.js"><link rel="prefetch" href="/assets/js/36.4ee62a31.js"><link rel="prefetch" href="/assets/js/37.cc538cd0.js"><link rel="prefetch" href="/assets/js/38.66fe6d97.js"><link rel="prefetch" href="/assets/js/39.62865922.js"><link rel="prefetch" href="/assets/js/4.ba8c4904.js"><link rel="prefetch" href="/assets/js/40.18d8b6ea.js"><link rel="prefetch" href="/assets/js/41.df0d8cab.js"><link rel="prefetch" href="/assets/js/42.2c76aec4.js"><link rel="prefetch" href="/assets/js/43.35b2ed84.js"><link rel="prefetch" href="/assets/js/44.7832dc93.js"><link rel="prefetch" href="/assets/js/45.bda8de55.js"><link rel="prefetch" href="/assets/js/46.6dfbc71d.js"><link rel="prefetch" href="/assets/js/47.ce3854ed.js"><link rel="prefetch" href="/assets/js/48.a0914b71.js"><link rel="prefetch" href="/assets/js/49.f409f02c.js"><link rel="prefetch" href="/assets/js/5.ebbfaeab.js"><link rel="prefetch" href="/assets/js/50.4f56073a.js"><link rel="prefetch" href="/assets/js/51.876c612d.js"><link rel="prefetch" href="/assets/js/52.d1cb790e.js"><link rel="prefetch" href="/assets/js/53.0e7bdacf.js"><link rel="prefetch" href="/assets/js/54.69554266.js"><link rel="prefetch" href="/assets/js/55.e91b1adb.js"><link rel="prefetch" href="/assets/js/56.629bc4a4.js"><link rel="prefetch" href="/assets/js/57.dd0e1f38.js"><link rel="prefetch" href="/assets/js/58.6210b964.js"><link rel="prefetch" href="/assets/js/59.3171311f.js"><link rel="prefetch" href="/assets/js/6.319d6800.js"><link rel="prefetch" href="/assets/js/60.f28df51c.js"><link rel="prefetch" href="/assets/js/61.9d91b4cf.js"><link rel="prefetch" href="/assets/js/62.64dca654.js"><link rel="prefetch" href="/assets/js/63.2422c5f1.js"><link rel="prefetch" href="/assets/js/64.2a1e3d2b.js"><link rel="prefetch" href="/assets/js/65.f58499a8.js"><link rel="prefetch" href="/assets/js/66.c0efeb82.js"><link rel="prefetch" href="/assets/js/67.661d7923.js"><link rel="prefetch" href="/assets/js/68.dfab90f4.js"><link rel="prefetch" href="/assets/js/69.d9aed373.js"><link rel="prefetch" href="/assets/js/7.b1af566c.js"><link rel="prefetch" href="/assets/js/70.42a55bd7.js"><link rel="prefetch" href="/assets/js/71.26f59ddc.js"><link rel="prefetch" href="/assets/js/72.08c4a8c5.js"><link rel="prefetch" href="/assets/js/73.3f37d91a.js"><link rel="prefetch" href="/assets/js/74.e351145c.js"><link rel="prefetch" href="/assets/js/75.f4de590b.js"><link rel="prefetch" href="/assets/js/76.67c626c7.js"><link rel="prefetch" href="/assets/js/77.f1130886.js"><link rel="prefetch" href="/assets/js/78.778d639c.js"><link rel="prefetch" href="/assets/js/8.3b56d639.js"><link rel="prefetch" href="/assets/js/80.4ee16963.js"><link rel="prefetch" href="/assets/js/81.01badefa.js"><link rel="prefetch" href="/assets/js/82.4613395c.js"><link rel="prefetch" href="/assets/js/83.03b7e3d1.js"><link rel="prefetch" href="/assets/js/84.a8ed7238.js"><link rel="prefetch" href="/assets/js/85.d0a23cae.js"><link rel="prefetch" href="/assets/js/86.47c348c3.js"><link rel="prefetch" href="/assets/js/87.7103da76.js"><link rel="prefetch" href="/assets/js/88.03092892.js"><link rel="prefetch" href="/assets/js/89.9e2e01ba.js"><link rel="prefetch" href="/assets/js/9.1ad63360.js"><link rel="prefetch" href="/assets/js/90.bc109fb6.js"><link rel="prefetch" href="/assets/js/91.edd4ee6f.js"><link rel="prefetch" href="/assets/js/92.47eef26d.js"><link rel="prefetch" href="/assets/js/93.f3dd468d.js"><link rel="prefetch" href="/assets/js/94.5928d8ae.js"><link rel="prefetch" href="/assets/js/95.a7a96583.js"><link rel="prefetch" href="/assets/js/96.b8df9976.js"><link rel="prefetch" href="/assets/js/97.3200a463.js"><link rel="prefetch" href="/assets/js/98.29d61ae7.js"><link rel="prefetch" href="/assets/js/99.25ff9892.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e4d8b86b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">JACKY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/" aria-current="page" class="sidebar-link">/</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Linux操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>专业课</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/深度学习/CNN.html" class="sidebar-link">CNN卷积神经网络</a></li><li><a href="/深度学习/ImageNet.html" class="sidebar-link">ImageNet</a></li><li><a href="/深度学习/RNN.html" class="sidebar-link">RNN</a></li><li><a href="/深度学习/SSD.html" class="sidebar-link">SSD目标检测</a></li><li><a href="/深度学习/manifold.html" class="sidebar-link">流形 Manifold</a></li><li><a href="/深度学习/优化器.html" class="sidebar-link">优化器</a></li><li><a href="/深度学习/归一化.html" class="sidebar-link">归一化方法</a></li><li><a href="/深度学习/数字图像处理.html" class="sidebar-link">数字图像处理</a></li><li><a href="/深度学习/欧拉增强的心率检测.html" class="sidebar-link">eulerian heartrate detection</a></li><li><a href="/深度学习/深度学习常见问题.html" class="sidebar-link">常见问题</a></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>论文</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/深度学习/论文/AlexNet.html" class="sidebar-link">AlexNet</a></li><li><a href="/深度学习/论文/BERT.html" class="sidebar-link">/深度学习/论文/BERT.html</a></li><li><a href="/深度学习/论文/GPT.html" class="sidebar-link">/深度学习/论文/GPT.html</a></li><li><a href="/深度学习/论文/GoogleNet.html" class="sidebar-link">GoogLeNet</a></li><li><a href="/深度学习/论文/Inception.html" class="sidebar-link">Inception</a></li><li><a href="/深度学习/论文/MAE.html" class="sidebar-link">MAE</a></li><li><a href="/深度学习/论文/ResNet.html" class="sidebar-link">ResNet</a></li><li><a href="/深度学习/论文/SAM.html" class="sidebar-link">SAM</a></li><li><a href="/深度学习/论文/VGG.html" class="sidebar-link">VGG</a></li><li><a href="/深度学习/论文/VIT.html" class="sidebar-link">VIT</a></li><li><a href="/深度学习/论文/attention.html" class="sidebar-link">/深度学习/论文/attention.html</a></li><li><a href="/深度学习/论文/mobileNet.html" class="sidebar-link">/深度学习/论文/mobileNet.html</a></li><li><a href="/深度学习/论文/segGPT.html" class="sidebar-link">SegGPT</a></li><li><a href="/深度学习/论文/segment anything.html" class="active sidebar-link">Segment Anything 论文解读</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#一-引言" class="sidebar-link">一 引言</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#二-task" class="sidebar-link">二 Task</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#三-model" class="sidebar-link">三 Model</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_1-image-encoder" class="sidebar-link">1.Image encoder</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_2-prompt-encoder" class="sidebar-link">2.Prompt encoder</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_3-mask-decoder" class="sidebar-link">3.Mask decoder</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_4-losses-and-training" class="sidebar-link">4.Losses and training</a></li></ul></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#四-data-engine" class="sidebar-link">四 Data Engine</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_1-assisted-manual-stage" class="sidebar-link">1.Assisted-manual stage</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_2-semi-automatic-stage" class="sidebar-link">2.Semi-automatic stage</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_3-fully-automatic-stage" class="sidebar-link">3.Fully automatic stage</a></li></ul></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#五-segment-anything-dataset" class="sidebar-link">五 Segment Anything Dataset</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#六-实验与效果" class="sidebar-link">六 实验与效果</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_1-零样本迁移-点分割验证" class="sidebar-link">1 零样本迁移——点分割验证</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_2-零样本迁移-边缘检测" class="sidebar-link">2 零样本迁移——边缘检测</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_3-零样本迁移-目标候选生成" class="sidebar-link">3 零样本迁移——目标候选生成</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_4-零样本迁移-实例分割" class="sidebar-link">4 零样本迁移——实例分割</a></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#_5-零样本迁移-文本引导的掩码生成" class="sidebar-link">5 零样本迁移——文本引导的掩码生成</a></li></ul></li><li class="sidebar-sub-header"><a href="/深度学习/论文/segment anything.html#七-总结和不足" class="sidebar-link">七 总结和不足</a></li></ul></li><li><a href="/深度学习/论文/shuffleNet.html" class="sidebar-link">/深度学习/论文/shuffleNet.html</a></li><li><a href="/深度学习/论文/transformer.html" class="sidebar-link">Transformer</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>硬件设计</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法与数据结构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程语言</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>联邦学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件开发</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="segment-anything-论文解读"><a href="#segment-anything-论文解读" class="header-anchor">#</a> Segment Anything 论文解读</h1> <p><img src="https://s2.loli.net/2023/05/02/kcB1G4tyOS3hPz8.png" alt="image-20230502114526789"></p> <p>于2023年4月5日发表在CVPR上，号称CV界的chatgpt，由meta公司（原facebook）发表</p> <p>PDF:<a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noopener noreferrer">Segment Anything (arxiv.org)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Code: <a href="https://github.com/facebookresearch/segment-anything" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/segment-anything<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Dataset: <a href="https://ai.facebook.com/datasets/segment-anything/" target="_blank" rel="noopener noreferrer">https://ai.facebook.com/datasets/segment-anything/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Demo的效果非常惊人: <a href="https://segment-anything.com/" target="_blank" rel="noopener noreferrer">https://segment-anything.com/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="一-引言"><a href="#一-引言" class="header-anchor">#</a> 一 引言</h2> <p><em><strong>论文试图解决什么问题？</strong></em></p> <ul><li>在图像分割领域建立一个<strong>基础模型</strong>, 使其可以实现非常强大的泛化能力, <strong>可以迁移到其他下游的图像分割任务中</strong></li> <li>计算机视觉领域非常广泛，对于其中许多问题，<strong>没有丰富的训练数据</strong></li></ul> <blockquote><p>“基础模型”:在大规模的广泛数据上训练，并适应广泛的下游任务的模型</p></blockquote> <p><em><strong>这是否是一个新的问题？</strong></em></p> <ul><li>不是，在NLP领域已经有了chatgpt等基于prompt的通用模型</li></ul> <p><em><strong>这篇文章要验证一个什么科学假设？</strong></em></p> <ul><li>分割任务也能通过prompt，来实现通用分割，进而实现基础通用视觉模型。</li></ul> <p><em><strong>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong></em></p> <ul><li>交互式分割、边缘检测、超像素化、目标建议生成、前景分割、语义分割、实例分割、全景分割等。</li> <li>本文工作与之前在多任务分割系统上的工作不同。在多任务系统中，单个模型执行一组固定的任务，例如联合语义、实例和全景分割，但训练和测试任务是相同的。本文工作中的一个重要区别是，为提示分割任务训练的模型可以作为更大系统中的一个组件，在推理时执行新的、不同的任务，例如，为了执行实例分割，将提示分割模型与现有的目标检测器相结合。</li></ul> <p><em><strong>论文中提到的解决方案之关键是什么？</strong></em></p> <p><img src="https://s2.loli.net/2023/05/02/Zbw2z5H6K41psxe.png" alt="image-20230502114734044"></p> <p>该计划的成功取决于三个组成部分：<strong>任务、模型和数据</strong>。 为了开发它们，作者解决了以下有关图像分割的问题：</p> <ol><li>什么任务可以实现零样本泛化？</li> <li>对应的模型架构是怎样的？</li> <li>哪些数据可以为这项任务和模型提供支持？</li></ol> <ul><li><strong>task</strong>：在NLP和计算机视觉中， Foundation models是一个有希望的发展，可以通过使用&quot;prompting&quot;技术对新数据集和任务进行零样本和少样本学习。受这一工作的启发，本文提出了<strong>promptable分割任务</strong>，目标是对于给定的任何提示，返回有效的分割mask。</li></ul> <p>这里的提示包括：点、框、mask、text</p> <img src="https://s2.loli.net/2023/05/02/wkB1K6tudFcySC5.png" alt="image-20230502120820491"> <ul><li><strong>model</strong>：通过将SAM分解为（heavyweight）<strong>图像编码器</strong>和（lightweight）<strong>快速prompt编码器/mask解码器</strong>，相同的图像嵌入可以在不同的提示符下重用。为了使SAM能够感知歧义，作者设计它为一个prompt预测多个掩码，由此SAM能够自然地处理歧义</li></ul> <p><img src="https://s2.loli.net/2023/05/02/mkoRayNiDXUYlP2.png" alt="image-20230502120829637"></p> <ul><li><strong>data</strong>：为了训练这个模型，需要一个多样化的、大规模的数据源——目前没有这样的数据源，作者构建了一个**“数据引擎”**，通过这个引擎可以高效获取想要的标注数据（左脚踩右脚）</li></ul> <p><img src="https://s2.loli.net/2023/05/02/59devfEHQZJ8lip.png" alt="image-20230502120855337"></p> <h2 id="二-task"><a href="#二-task" class="header-anchor">#</a> <strong>二 Task</strong></h2> <p>启发来源：NLP领域、Prompt Engineering</p> <p><strong>Task：</strong></p> <p>将prompt的想法从NLP领域转到图像分割领域，prompt可以是一组前景/背景点、一个粗糙的框或mask、文本等等。<strong>promptable分割任务</strong>是在任何prompt下返回一个<strong>有效的分割mask</strong>。“有效”mask的要求仅仅意味着，即使一个prompt是不明确的，输出也应该是这些对象中至少一个合理的mask。</p> <p><strong>Pre-training：</strong></p> <p>输入的无论是图像还是提示，首先需要转化成一个向量，作者在这里直接用现成的（<strong>ViT、CLIP</strong>）</p> <p>promptable分割任务提出了一种自然的预训练算法，该算法为每个训练样本模拟一系列prompts(例如点、框、掩码)，并将模型的mask预测与GT进行比较。作者将这种方法应用于<strong>交互式分割</strong>（低成本的数据标注），目的是始终预测任何prompt的有效掩码，即使prompt是模糊的。</p> <p><strong>Zero-shot transfer：</strong></p> <p>预训练任务赋予了模型在推理时对任何prompt作出适当反应的能力，从而可以通过设计适当的prompts来解决下游任务。例如，如果有一个猫的bounding box detector，那么可以通过向我们的模型提供detector's box输出作为prompt来解决猫实例分割问题。</p> <p><strong>Discussion</strong></p> <p>作者认为：<strong>提示</strong>和<strong>组合</strong>是强大的工具，使单个模型能够以可扩展的方式使用，可完成模型设计时未知的任务。</p> <p>由prompt等驱动的可组合系统将比专门为固定任务训练的系统更广泛地支持各种应用。</p> <h2 id="三-model"><a href="#三-model" class="header-anchor">#</a> 三 Model</h2> <p>SAM有三个部分组成，如下图所示，分别是图像encoder，灵活的提示encoder和快速掩码decoder。建立在Vision Transformer上，对实时性能进行特定的权衡</p> <p><img src="https://s2.loli.net/2023/05/02/IfT6zt24gA8Gjhx.png" alt="image-20230502114826415"></p> <h3 id="_1-image-encoder"><a href="#_1-image-encoder" class="header-anchor">#</a> 1.Image encoder</h3> <p>使用<strong>MAE预训练的ViT</strong>（heavyweight），最小限度地适用于处理<strong>高分辨率输入</strong>(输入图像基本都是2k左右的)。</p> <p>图像编码器对每张图像运行一次，在提示模型之前进行应用。</p> <ol><li>对于输入的图像，通过缩放和pad较短边将其尺寸改为1024×1024（C×H×W→C×1024×1024）。</li> <li>经过image encoder，得到对图像16倍下采样的feature，<strong>最终的大小为(256,64,64)</strong>。</li></ol> <h3 id="_2-prompt-encoder"><a href="#_2-prompt-encoder" class="header-anchor">#</a> 2.Prompt encoder</h3> <p>考虑两种提示：<strong>稀疏</strong>(点、框、文本)和<strong>密集</strong>(mask)。</p> <img src="https://s2.loli.net/2023/05/02/wkB1K6tudFcySC5.png" alt="image-20230502120820491"> <ul><li><strong>point（稀疏）</strong>：映射到256维的向量，包含代表点位置的 positional encoding，加2个代表该点是前景/背景的可学习的embedding。</li> <li><strong>box（稀疏）</strong>：用一个embedding对表示（1）可学习的embedding代表左上角（2）可学习的embedding代表右下角</li> <li>**文本（稀疏）：**使用预训练的CLIP模型中的text encoder进行embedding</li> <li>**mask （稠密）：**用三个卷积进行嵌入（考虑稠密信息之间的空间关系，所以不能直接使用embedding），之后mask 和image embedding通过element-wise逐元素求和(<strong>可以理解成mask的feature对image的feature进行加权</strong>)</li></ul> <p>不管是那种prompt，或者是其中的任意组合，最后Prompt Encoder都是输出256维的一个向量。</p> <p>代码：</p> <div class="language-python extra-class"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        points<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># 要嵌入的点坐标和标签。</span>
        boxes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
        masks<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
       
        bs <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_batch_size<span class="token punctuation">(</span>points<span class="token punctuation">,</span> boxes<span class="token punctuation">,</span> masks<span class="token punctuation">)</span>
        sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_get_device<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> points <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            coords<span class="token punctuation">,</span> labels <span class="token operator">=</span> points <span class="token comment">#坐标，标签</span>
            point_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_points<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> pad<span class="token operator">=</span><span class="token punctuation">(</span>boxes <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>sparse_embeddings<span class="token punctuation">,</span> point_embeddings<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> boxes <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            box_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_boxes<span class="token punctuation">(</span>boxes<span class="token punctuation">)</span>
            sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>sparse_embeddings<span class="token punctuation">,</span> box_embeddings<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> masks <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            dense_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_masks<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dense_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>no_mask_embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
                bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>image_embedding_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>image_embedding_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token punctuation">)</span>

        <span class="token keyword">return</span> sparse_embeddings<span class="token punctuation">,</span> dense_embeddings
</code></pre></div><p>point embed的代码：</p> <div class="language-python extra-class"><pre class="language-python"><code>input_point <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">500</span><span class="token punctuation">,</span> <span class="token number">375</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
input_label <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_embed_points</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        points<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>  
        labels<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>  <span class="token comment"># 1—前景；0—背景</span>
        pad<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;Embeds point prompts.&quot;&quot;&quot;</span>
        points <span class="token operator">=</span> points <span class="token operator">+</span> <span class="token number">0.5</span>  <span class="token comment"># Shift to center of pixel</span>
        <span class="token keyword">if</span> pad<span class="token punctuation">:</span>
            <span class="token comment"># 建立一个和point的shape相同的空向量</span>
            padding_point <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>points<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            <span class="token comment"># 建立一个和labels的shape相同的-1向量</span>
            padding_label <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>labels<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            <span class="token comment"># pointshape由(1,2)--(2,2)</span>
            points <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>points<span class="token punctuation">,</span> padding_point<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># labelshape由(1)--(2)</span>
            labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>labels<span class="token punctuation">,</span> padding_label<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对点进行位置编码                </span>
        point_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>pe_layer<span class="token punctuation">.</span>forward_with_coords<span class="token punctuation">(</span>points<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_image_size<span class="token punctuation">)</span>
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>  
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>not_a_point_embed<span class="token punctuation">.</span>weight <span class="token comment"># 这里不是前景背景，是填充</span>
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight <span class="token comment">#背景</span>
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight <span class="token comment">#前景</span>
        <span class="token keyword">return</span> point_embedding <span class="token comment"># (2,256)</span>
</code></pre></div><p>box embed的代码：</p> <div class="language-python extra-class"><pre class="language-python"><code>input_box <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">425</span><span class="token punctuation">,</span> <span class="token number">600</span><span class="token punctuation">,</span> <span class="token number">700</span><span class="token punctuation">,</span> <span class="token number">875</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_embed_boxes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> boxes<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;Embeds box prompts.&quot;&quot;&quot;</span>
        boxes <span class="token operator">=</span> boxes <span class="token operator">+</span> <span class="token number">0.5</span>  <span class="token comment"># Shift to center of pixel</span>
        coords <span class="token operator">=</span> boxes<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment">#shape(1,4)—&gt;(2,2)</span>
        <span class="token comment"># 对box进行位置编码 </span>
        corner_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>pe_layer<span class="token punctuation">.</span>forward_with_coords<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_image_size<span class="token punctuation">)</span> <span class="token comment">#(2,256)</span>
        <span class="token comment"># 分别对背景前景加权</span>
        corner_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        corner_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        <span class="token keyword">return</span> corner_embedding
</code></pre></div><p>mask embed的代码：</p> <div class="language-python extra-class"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">_embed_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> masks<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;Embeds mask inputs.&quot;&quot;&quot;</span>
        mask_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>mask_downscaling<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
        <span class="token keyword">return</span> mask_embedding

<span class="token comment"># -----------------------------------------------------------</span>
self<span class="token punctuation">.</span>mask_downscaling <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            LayerNorm2d<span class="token punctuation">(</span>mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> mask_in_chans<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            LayerNorm2d<span class="token punctuation">(</span>mask_in_chans<span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mask_in_chans<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="_3-mask-decoder"><a href="#_3-mask-decoder" class="header-anchor">#</a> 3.Mask decoder</h3> <p>输入是 Image Encoder 和 Prompt Encoder 的输出。TwoWayTransformer 是指 transformer 的 v 不变，有两套输入的 q 和 k。做完 Attention 之后，Mask Decoder 会做上采样，然后用 MLP 预测最后的 mask_iou。</p> <p><img src="https://s2.loli.net/2023/05/02/8mQKiCDcsaglnZb.png" alt="image-20230502140631940"></p> <ol><li><p>作者修改了Transformer decoder，在使用decoder之前，首先<strong>将可学习的output token嵌入到prompt embedding集合中</strong>（类似于vit中的[class]标记）。</p></li> <li><p>修改后的解码器块使用两个方向上的提示<strong>自注意力</strong>和<strong>交叉注意力</strong>(提示到图像嵌入，图像嵌入再到提示)来更新所有嵌入。每个解码器执行四个步骤，重复两次</p> <p>①prompt tokens+output tokens进行自注意力（这里prompt tokens会输入两遍！！！使decoder对prompt token的几何位置和类型都有很强的依赖）</p> <p>②用得到的token和image embedding进行cross-attention（token作为Q）</p> <p>③逐点MLP 更新token</p> <p>④用image embedding和③的token进行cross-attention（image embedding作为Q）</p></li></ol> <p>重复上述步骤2次，再将attn再通过残差进行连接，最终输出masks和iou scores。</p> <ol start="3"><li><p>运行解码器后，使用两个卷积层对image embedding进行4倍的上采样(相对于输入图像缩小了4倍，信息扩充256*256)。</p></li> <li><p>token再次参与图像嵌入，将更新后的output token embedding传递给一个小的3层MLP，该MLP输出与放大图像嵌入的通道维度匹配的向量。最后，用放大后的image embedding和MLP输出的逐点乘积，来计算每个图像位置的mask概率。</p></li></ol> <p>另外，为了解决输出模糊性问题(一个提示可能生成多个mask，比如衣服上的一个点，既可以表示衣服，也表示穿衣服的人)</p> <img src="https://s2.loli.net/2023/05/02/6NHyagmIBRr8qlG.png" alt="image-20230502140846284" style="zoom:80%;"> <p>本文采取的策略是，<strong>使用少量output tokens同时预测多个mask</strong>，而不是预测单个mask。默认预测三个mask(整体、部分和子部分)，在训练过程中计算GT和每个预测mask之间的损失，但只回传最小的loss。另外，为了对mask进行排序，添加了一个小head(在额外的output token上操作)，以估计每个mask与其覆盖的对象之间的IoU。</p> <h3 id="_4-losses-and-training"><a href="#_4-losses-and-training" class="header-anchor">#</a> 4.Losses and training</h3> <p>mask loss ： focal loss和dice loss的线性组合，focal loss与dice loss的比例为20:1；</p> <p>iou loss：  用mse loss</p> <p>We train for the promptable segmentation task <strong>using a mixture of geometric prompts</strong>， we simulate an interactive setup by randomly sampling prompts in <strong>11 rounds per mask</strong>, al- lowing SAM to integrate seamlessly into our data engine.（形成11种mask和prompt的对）</p> <h2 id="四-data-engine"><a href="#四-data-engine" class="header-anchor">#</a> <strong>四 Data Engine</strong></h2> <p>该数据引擎有三个阶段:</p> <ol><li>模型辅助的<strong>手动标注阶段</strong>，</li> <li><strong>半自动阶段</strong>，其中混合了自动预测的掩码和模型辅助的标注，</li> <li><strong>全自动阶段</strong>，在该阶段中，我们的模型在没有人工辅助输入的情况下生成mask。</li></ol> <h3 id="_1-assisted-manual-stage"><a href="#_1-assisted-manual-stage" class="header-anchor">#</a> <strong>1.Assisted-manual stage</strong></h3> <p>一组专业标注人员通过使用SAM支持的基于浏览器的交互式分割工具点击前景/背景的点来标记mask，人工对mask进行细化。模型可以实时输出mask，建议标注者优先标记他们命名的对象。另外，要求标注者按图层顺序标记物体，如果一个mask标记超过30s，先处理下一张。</p> <p>SAM先用公开数据集训练，然后再用新增的标注mask训练。随着收集到更多的mask，图像编码器从ViT-B扩展到ViT-H，其他架构细节也得到了细化，image-encoder的能力越来越强。总共重新<strong>训练了模型6次</strong>。随着模型的改进，每个mask的平均标注时间从34秒减少到14秒（<strong>通过交互式分割来简化标注工作</strong>），每个图像的平均mask数量从20个mask增加到44个mask。</p> <p>总的来说，在这个阶段<strong>从12万张图像中收集了430万个mask。</strong></p> <h3 id="_2-semi-automatic-stage"><a href="#_2-semi-automatic-stage" class="header-anchor">#</a> <strong>2.Semi-automatic stage</strong></h3> <p>这一阶段的目标是<strong>增加mask的多样性</strong></p> <p>为了使标注者关注到不太突出的对象，首先自动检测出可信的mask。然后向标注者展示了预测mask填充图像，并让他们标注其他没被标注的对象。</p> <p>为了检测可信的mask，先<strong>用第一步的mask训练了一个类别一样的box检测器</strong>。在此阶段，从18万张图像中收集了额外的590万个mask(此时总共已有1020万个mask)。与第一阶段一样，用新收集的数据，<strong>重新训练模型(5次)</strong>。每个mask的平均注释时间回到34秒，因为这些对象更难标记。每个图像的平均掩码数量从44个增加到72个。</p> <h3 id="_3-fully-automatic-stage"><a href="#_3-fully-automatic-stage" class="header-anchor">#</a> <strong>3.Fully automatic stage</strong></h3> <p>在这一阶段的开始，已经通过前两个阶段收集了大量的和多样性的mask来改进模型。另外，到此阶段，我们已经开发了<strong>模糊感知模型</strong>，它可以根据不明确的输入也能输出有效的mask。</p> <p>具体来说，<strong>对图像生成（32,32）个格网点，每个点预测一系列mask</strong>，如果一个点落在某个部分或子部分上，模型返回部分、子部分和整体的object。同时，通过预测的iou筛选<strong>confident</strong>(可信的mask),选取一个<strong>stable</strong>的mask(稳定的mask,在相似的mask中，概率阈值在 0.5-δ和 0.5+δ之间)；最后，通过nms过滤<strong>confident</strong>和<strong>stable</strong>中重复的mask。</p> <p>至此，在所有1100万幅图像上，总共产生了11亿个高质量的mask。</p> <h2 id="五-segment-anything-dataset"><a href="#五-segment-anything-dataset" class="header-anchor">#</a> <strong>五 Segment Anything Dataset</strong></h2> <p>所提出的数据集SA-1B由1100万不同的、高分辨率的、授权的、保护隐私的图像和用数据引擎收集的11亿个高质量mask组成。平均每张图像有100个mask</p> <img src="https://s2.loli.net/2023/05/02/TEvwZBhsWDiqHkR.png" alt="image-20230502190525265" style="zoom:67%;"> <p><strong>图像</strong>：从一家直接与摄影师合作的提供商获取1100万张高分辨率的(平均3300×4950像素)图像，按短边重采样到1500像素。即使在下采样后的图像分辨率也明显高于许多现有的视觉数据集(例如，COCO图像是480×640像素)。</p> <p><strong>mask</strong>：数据引擎生产了11亿个掩码，其中99.1%都是自动生成的。通过对比分析，自动生成的mask质量也是非常高的。</p> <p><strong>Mask quality</strong>：为了评估质量，随机选500张图像（约5万个mask），让专业的标注人员进行标注，计算了每对之间的IoU，发现94%的mask对的IoU大于90%(97%的对的IoU大于75%)。作为比较，之前的工作估计标注者之间的一致性在85-91% IoU。</p> <p><strong>Mask properties</strong>： 数据分布更广，从全世界获取数据，mask更多，数据偏向性较小。</p> <p><img src="https://s2.loli.net/2023/05/02/AT5mi8FXjEUfLQH.png" alt="image-20230502191030379"></p> <h2 id="六-实验与效果"><a href="#六-实验与效果" class="header-anchor">#</a> <strong>六 实验与效果</strong></h2> <p><strong>核心：验证其零样本的迁移能力</strong></p> <p>作者考虑五个任务，其中四个与用于训练 SAM 的可提示分割任务有很大不同。这些实验在训练期间未见的数据集和任务上评估 SAM（对“零样本迁移”的使用遵循其在 CLIP 中的使用）。数据集可能包括新颖的图像分布，例如水下或以第一视角的图像，这些图像不会出现在 SA-1B 中。</p> <p>实验首先测试可提示分割的核心目标：从任何提示生成有效的掩码。</p> <ol><li>执行<strong>边缘检测；</strong></li> <li>分割所有内容，即<strong>对象候选生成；</strong></li> <li>分割检测到的对象，即<strong>实例分割；</strong></li> <li>作为概念验证，以分割来自自由格式文本的对象（根据文本分割）。</li></ol> <p>这四个任务与 SAM 接受训练并通过提示工程实现的提示分割任务有很大不同。</p> <p>实施细节：除非另有说明，SAM 使用 <strong>MAE 预训练的 ViT-H 图像编码器</strong></p> <h3 id="_1-零样本迁移-点分割验证"><a href="#_1-零样本迁移-点分割验证" class="header-anchor">#</a> 1 零样本迁移——点分割验证</h3> <p>评估从单个前景点分割对象（使用groud truth的中心点作为输入），使用muti-mask中的confidence mask</p> <p>作者用一项<strong>人类评估</strong>来补充标准的 mIoU 指标（即，预测掩码和真实掩码之间所有 <strong>IoU 的平均值</strong>），其中注释者将掩码质量从 1（无意义）到 10（像素完美）进行评分。</p> <p>我们主要与 RITM分割算法进行比较，下图是对比的实验结果：</p> <p><img src="https://s2.loli.net/2023/05/02/OqJtHC2AD87vw6g.png" alt="image-20230502191052428"></p> <p>SAM 在 23 个数据集中的 16 个上产生了更高的结果，高达 ∼47 IoU</p> <h3 id="_2-零样本迁移-边缘检测"><a href="#_2-零样本迁移-边缘检测" class="header-anchor">#</a> 2 零样本迁移——边缘检测</h3> <img src="https://s2.loli.net/2023/05/02/dXlLKBYW1NA4Gpn.png" alt="image-20230502194441775" style="zoom:67%;"> <p><img src="https://s2.loli.net/2023/05/02/1gtIaT9Fv32ACWB.png" alt="image-20230502191109133"></p> <p>定性地，可以观察到即使 SAM 没有接受过边缘检测训练，它也会产生合理的边缘图。 与 ground truth 相比，SAM 预测了更多的边缘，包括 BSDS500 中未注释的敏感边缘（<strong>准确率低，召回率高</strong>）</p> <p><img src="https://s2.loli.net/2023/05/02/ysan6EzFBgQRjYv.png" alt="image-20230502191126306"></p> <h3 id="_3-零样本迁移-目标候选生成"><a href="#_3-零样本迁移-目标候选生成" class="header-anchor">#</a> 3 零样本迁移——目标候选生成</h3> <p>目标候选生成在目标检测研究中发挥了重要作用，作为开创性系统的中间步骤。为了生成对象候选，论文运行了一个稍微修改过的自动掩码生成管道版本。数据集使用 LVIS，因为它的大量类别提出了具有挑战性的测试。 SAM与作ViTDet检测器（使用级联 Mask R-CNN ViT-H）实现的强基线进行比较。注意到，这个“基线”对应于向游戏 AR 展示的“检测器伪装成提议生成器”(DMP) 方法，使其成为一个真正苛刻的比较。</p> <p><img src="https://s2.loli.net/2023/05/02/QmeJ65TvNhgFtbP.png" alt="image-20230502191138629"></p> <p>在上表中，不出所料地看到使用 ViTDet-H 的检测作为对象候选总体上表现最好。 然而，SAM 在几个指标上表现非常出色。 值得注意的是，它在中型和大型物体以及稀有和常见物体上的表现优于 ViTDet-H。 事实上，SAM 仅在小对象和频繁对象上表现不如 ViTDet-H，而 ViTDet-H 可以轻松学习 LVIS 特定的注释偏差，因为它是在 LVIS 上训练的，与 SAM 不同。 作者还与 SAM 的消除歧义无意识版本（“单挑”）进行了比较，后者在所有 AR 指标上的表现都明显低于 SAM。</p> <h3 id="_4-零样本迁移-实例分割"><a href="#_4-零样本迁移-实例分割" class="header-anchor">#</a> 4 零样本迁移——实例分割</h3> <p>转向更高层次的愿景，使用 SAM 作为实例分割器的分割模块。 实现很简单：运行对象检测器（之前使用的 ViTDet）并使用其输出框提示 SAM。 这说明了在更大的系统中编写 SAM。</p> <p><img src="https://s2.loli.net/2023/05/02/FQETbC7f4g3aWrm.png" alt="image-20230502191151386"></p> <p>观察到两个数据集上的差距，其中 SAM 相当接近，但肯定落后于 ViTDet。假设在 COCO 上，mask AP 间隙较大且标注质量相对较低（正如人类研究所证实的那样），ViTDet 学习了 COCO masks 的特定偏差。SAM 作为一种零样本方法，无法利用这些（通常不受欢迎的）偏差。LVIS 数据集具有更高质量的 ground truth，但仍然存在特定的特性（例如，masks 不包含孔，它们是构造简单的多边形）和模态与 amodal masks 的偏差。 同样，SAM 没有接受过学习这些偏差的训练，而 ViTDet 可以利用它们。</p> <h3 id="_5-零样本迁移-文本引导的掩码生成"><a href="#_5-零样本迁移-文本引导的掩码生成" class="header-anchor">#</a> 5 零样本迁移——文本引导的掩码生成</h3> <p>考虑一个更高级别的任务：从自由格式文本中分割对象。该实验是 SAM 处理文本提示的能力的概念验证。 虽然在之前的所有实验中都使用了完全相同的 SAM，但对于这个 SAM 的训练过程进行了修改，使其具有文本感知能力，但不需要新的文本注释。 具体来说，对于每个面积大于 100^2 的手动收集掩码，我们提取 CLIP 图像嵌入。 然后，在训练期间，我们使用提取的 CLIP 图像嵌入提示 SAM 作为其第一次交互。</p> <p>因为 <strong>CLIP 的图像嵌入经过训练以与其文本嵌入对齐</strong>，所以我们可以使用图像嵌入进行训练，但使用文本嵌入进行推理。 也就是说，在推理时，我们通过 CLIP 的文本编码器运行文本，然后将生成的文本嵌入作为 SAM 的提示。</p> <p><img src="https://s2.loli.net/2023/05/02/r3D2sZBOdW7P1Yc.png" alt="image-20230502191204160"></p> <h2 id="七-总结和不足"><a href="#七-总结和不足" class="header-anchor">#</a> 七 总结和不足</h2> <p>Segment Anything 项目贡献：<strong>新任务（可提示分割）、模型（SAM）和数据集（SA-1B）</strong>。</p> <p><strong>不足</strong>：</p> <ol><li>虽然SAM总体上表现良好，但并不完美，它可能会错过精细的结构;</li> <li>专用的分割方法在一些任务上性能优于SAM，比如医学分割;</li> <li>文中对text-to-mask任务的尝试是探索性的，并不完全可靠;</li></ol> <p><strong>未来：</strong></p> <ol><li>预训练模型可以提供新功能，甚至超出训练时的想象（<strong>涌现</strong>现象）。一个突出的例子是 CLIP如何用作更大系统中的一个组件，例如 DALL·E。</li> <li>本文的目标是使用 SAM 使这种组合变得简单明了，通过要求 SAM 为广泛的分割提示预测有效掩码来实现这一目标。 效果是在 SAM 和其他组件之间创建可靠的接口。 例如，MCC可以轻松地使用 SAM 来分割感兴趣的对象，并实现对未见对象的强泛化，以便从单个 RGB-D 图像进行 3D 重建。</li> <li>在另一个示例中，SAM 可以通过可穿戴设备检测到的注视点进行提示，从而启用新的应用程序。 由于 SAM 能够泛化到第一视角的图像等新领域，这样的系统无需额外培训即可工作。</li></ol></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">最后更新时间:</span> <span class="time">1 分钟前</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/深度学习/论文/segGPT.html" class="prev">
        SegGPT
      </a></span> <span class="next"><a href="/深度学习/论文/shuffleNet.html">
        /深度学习/论文/shuffleNet.html
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.6d855320.js" defer></script><script src="/assets/js/2.d4ff5c60.js" defer></script><script src="/assets/js/79.86f9f4d3.js" defer></script>
  </body>
</html>
