(window.webpackJsonp=window.webpackJsonp||[]).push([[58],{480:function(t,s,_){"use strict";_.r(s);var a=_(65),e=Object(a.a)({},(function(){var t=this,s=t.$createElement,_=t._self._c||s;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h1",{attrs:{id:"rnn"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rnn"}},[t._v("#")]),t._v(" RNN")]),t._v(" "),_("h2",{attrs:{id:"_0-文本预处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_0-文本预处理"}},[t._v("#")]),t._v(" 0 文本预处理")]),t._v(" "),_("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://s2.loli.net/2023/02/26/qOmgnesoLvtkxXY.png",alt:"image-20230226171802884"}}),t._v(" "),_("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://s2.loli.net/2023/02/26/cqgvf4QGhosxUbJ.png",alt:"image-20230226172026407"}}),t._v(" "),_("p",[_("strong",[t._v("step1：文本处理text to sequence")])]),t._v(" "),_("ol",[_("li",[_("p",[t._v("tokenization（text to words），把一个文本分成一个个的单词")])]),t._v(" "),_("li",[_("p",[t._v("count word frequency（统计词频）：保留常用词，去除低频词（名字、错误词），把每个单词映射成一个正整数")])]),t._v(" "),_("li",[_("p",[t._v("one-hot encoding")]),t._v(" "),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2023/02/26/Y83BFGyMPVlnHNi.png",alt:"image-20230226162358394"}})]),t._v(" "),_("li",[_("p",[t._v("align sequence")]),t._v(" "),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2023/02/26/Ci6Yg5M2cxPZpk3.png",alt:"image-20230226162630587"}})])]),t._v(" "),_("p",[_("strong",[t._v("step2：word embedding（word to vector）")])]),t._v(" "),_("p",[t._v("在step1中的 align sequence之后（假设都对其到 input_length  = 20 ），那么每个样例的输入长度都是20个words；")]),t._v(" "),_("p",[t._v("但是我们需要对这words转为vector，可以直接想到的是用one-hot方法，对于我们得到的vocabulary（词汇量，字典，一般很大，比如10k），那么使用one-hot编码，那么得到的vector也会很大，一个单词就是10k维度。")]),t._v(" "),_("p",[t._v("所以我们采用word embedding（他的思路就是，把one-hot向量给降维，或者说是做了一个线性变换，就是乘上一个矩阵）")]),t._v(" "),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2023/02/26/K4rUqtBhFTgInO3.png",alt:"image-20230226170434127"}}),t._v(" "),_("p",[t._v("这里的d是最后我们对于每个单词表示的词向量的维度（embedding_dim），v是vocabulary的词汇量大小")]),t._v(" "),_("p",[t._v("embedding层的参数 =  vocabulary * embedding_dim")]),t._v(" "),_("h2",{attrs:{id:"_1-序列数据"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-序列数据"}},[t._v("#")]),t._v(" 1 序列数据")]),t._v(" "),_("p",[t._v("序列数据：前后数据通常具有"),_("strong",[t._v("关联性")])]),t._v(" "),_("h2",{attrs:{id:"_2-语言模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-语言模型"}},[t._v("#")]),t._v(" 2 语言模型")]),t._v(" "),_("p",[t._v("NLP")]),t._v(" "),_("p",[t._v("时间步：一个单词算一个time step；")]),t._v(" "),_("p",[t._v("联合概率：")]),t._v(" "),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/02/LEol7YsvWt1mQzk.png",alt:"image-20220502111757867"}}),t._v(" "),_("p",[t._v("以上的概率，由语料库中的词频统计来得到。")]),t._v(" "),_("p",[t._v("缺点：第i个词，依赖第i-1个词。随着时间步的增大，计算量呈指数递增。")]),t._v(" "),_("h2",{attrs:{id:"_3-循环神经网络"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-循环神经网络"}},[t._v("#")]),t._v(" 3 循环神经网络")]),t._v(" "),_("p",[t._v("​\t\tRNN是针对序列数据而生的神经网络结构，核心在于循环使用网络层参数，避免时间步增大带来的参数激增，并引入"),_("strong",[t._v("隐藏状态")]),t._v("，用于记录历史信息，有效的处理数据的前后关联性。")]),t._v(" "),_("p",[_("img",{attrs:{src:"https://s2.loli.net/2023/02/26/9Xn6FMSdtZ5W2mN.png",alt:"image-20230226172817676"}})]),t._v(" "),_("p",[_("img",{attrs:{src:"https://s2.loli.net/2022/05/03/ZWqQodMPRl4D2XE.png",alt:"image-20220503114850772"}})]),t._v(" "),_("p",[t._v("三个权重\\boldsymbol{W}_{x h}、\\boldsymbol{W}_{h h}、\\boldsymbol{W}_{h q}，会循环使用。")]),t._v("\n\\begin{array}{l}\n\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right) \\\\\n\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n\\end{array}\n\n"),_("p",[t._v("这里\\boldsymbol{X}_{t}是在时间步为t时的"),_("strong",[t._v("输入数据")]),t._v("，而\\boldsymbol{O}_{t}是时间步为t时的"),_("strong",[t._v("输出数据")]),t._v("；其中的\\boldsymbol{H}_{t}为时间步为t的"),_("strong",[t._v("隐藏状态")]),t._v("（用于记录历史信息）")]),t._v(" "),_("p",[t._v("其中"),_("code",[t._v("隐藏状态")]),t._v("的作用是：用于记录历史信息，有效处理数据的前后关联性。激活函数采用==Tanh==（也就是上述公式中的$ \\phi $），将输出值域控制在（-1,1），防止数值呈指数级变化。")]),t._v(" "),_("p",[t._v("RNN特性：")]),t._v(" "),_("ol",[_("li",[t._v("循环神经网络的==隐藏状态==可以捕捉截至当前时间步的序列的历史信息")]),t._v(" "),_("li",[t._v("循环神经网络模型参数的数量不随时间步的增加而增长")])]),t._v(" "),_("p",[t._v("问题：")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("为什么simple RNN 使用tanh函数")]),t._v(" "),_("p",[t._v("由于RNN 的ht的计算依赖于上一次的ht-1，可以简单理解为是一个累乘的过程，所以对于参数矩阵A中的参数，很容易出现参数累乘后爆炸、或者参数累乘后消失的情况。通过tanh函数，就可以把每一次得到的新ht的参数范围控制在-1~1之间，从而避免ht中参数过大或者消失的情况。")])]),t._v(" "),_("li",[_("p",[t._v("Simple RNN的缺点")]),t._v(" "),_("p",[t._v("simple RNN只能记住短序列的，对于长序列的输入，后面的隐藏状态会遗忘前面的输入。LSTM则比simple RNN的记忆时间长很多")])])]),t._v(" "),_("h3",{attrs:{id:"_3-1-循环神经网络的反向传播"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-循环神经网络的反向传播"}},[t._v("#")]),t._v(" 3.1 循环神经网络的反向传播")]),t._v(" "),_("p",[t._v("穿越时间的反向传播")]),t._v(" "),_("p",[t._v("致命"),_("strong",[t._v("缺点")]),t._v("：梯度随时间t呈指数变化（主要是看参数\\boldsymbol{W}_{hh}），容易引发"),_("code",[t._v("梯度消失")]),t._v("、"),_("code",[t._v("梯度爆炸")])]),t._v(" "),_("h2",{attrs:{id:"_4-gru-门控循环单元"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-gru-门控循环单元"}},[t._v("#")]),t._v(" 4 GRU-门控循环单元")]),t._v(" "),_("p",[t._v("引入了"),_("strong",[t._v("门")]),t._v("的循环神经网络——GRU")]),t._v(" "),_("p",[t._v("对于梯度爆炸，我们可以采用"),_("strong",[t._v("梯度裁剪")]),t._v("的方式去解决；但是对于梯度消失，我们无法解决。")]),t._v(" "),_("h3",{attrs:{id:"_4-1-梯度裁剪"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-梯度裁剪"}},[t._v("#")]),t._v(" 4.1 梯度裁剪")]),t._v(" "),_("p",[t._v("梯度裁剪是解决梯度爆炸的一种技术，其出发点是非常简明的：如果梯度变得非常大，那么我们就调节它使其保持较小的状态。精确的说，如果∥ g ∥ ≥ c ，则")]),t._v("\ng ← c ⋅ g / ∥ g ∥\n\n"),_("p",[t._v("​\t\t此处的c指超参数，"),_("span",{staticClass:"katex"},[_("span",{staticClass:"katex-mathml"},[_("math",[_("semantics",[_("mrow",[_("mi",[t._v("g")])],1),_("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("g")])],1)],1)],1),_("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[_("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),_("span",{staticClass:"strut bottom",staticStyle:{height:"0.625em","vertical-align":"-0.19444em"}}),_("span",{staticClass:"base textstyle uncramped"},[_("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")])])])]),t._v("指梯度，∥ g ∥为梯度的"),_("a",{attrs:{href:"https://so.csdn.net/so/search?q=%E8%8C%83%E6%95%B0&spm=1001.2101.3001.7020",target:"_blank",rel:"noopener noreferrer"}},[t._v("范数"),_("OutboundLink")],1),t._v("，g / ∥ g ∥必然是个单位矢量，因此在进行调节后新的梯度范数必然等于c，注意到如果∥ g ∥ ≤ c则不需要进行调节。\n  梯度裁剪确保了梯度矢量的最大范数（本文中规定为c）。即使在模型的损失函数不规则时，这一技巧也有助于梯度下降保持合理的行为。下面的图片展示了损失函数的陡崖。不采用裁剪，参数将会沿着梯度下降方向剧烈变化，导致其离开了最小值范围；而使用裁剪后参数变化将被限制在一个合理范围内，避免了上面的情况。")]),t._v(" "),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/04/phYExNtbcVFdOoz.png",alt:"在这里插入图片描述"}}),t._v(" "),_("h3",{attrs:{id:"_4-2-重置门-更新门"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-重置门-更新门"}},[t._v("#")]),t._v(" 4.2 重置门&&更新门")]),t._v(" "),_("p",[t._v("作用：缓解RNN"),_("strong",[t._v("梯度消失")]),t._v("带来的问题，引入门的概念，来控制信息流动，使模型更好的记住长远时期的信息，并缓解梯度消失。")]),t._v(" "),_("p",[t._v("重置门：哪些信息需要遗忘")]),t._v(" "),_("p",[t._v("更新门：哪些信息需要注意")]),t._v(" "),_("img",{staticStyle:{zoom:"53%"},attrs:{src:"https://s2.loli.net/2022/05/04/E65tH8kD1TGeu3j.png",alt:"image-20220504090846784"}}),t._v("\n$$\n\\begin{array}{l}\n重置门：\\boldsymbol{R}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x r}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h r}+\\boldsymbol{b}_{r}\\right) \\\\\n更新门：\\boldsymbol{Z}_{t}=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x z}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h z}+\\boldsymbol{b}_{z}\\right)\n\\end{array}\n$$\n"),_("p",[t._v("这里的"),_("span",{staticClass:"katex"},[_("span",{staticClass:"katex-mathml"},[_("math",[_("semantics",[_("mrow",[_("mi",[t._v("σ")])],1),_("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sigma")])],1)],1)],1),_("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[_("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),_("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),_("span",{staticClass:"base textstyle uncramped"},[_("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("σ")])])])]),t._v("是激活函数，此处采用的是sigmoid激活函数，使门的值为（0,1），0表示遗忘，1表示保留。")]),t._v(" "),_("p",[t._v("GRU和传统RNN的区别：")]),t._v(" "),_("blockquote",[_("p",[t._v("GRU引入了门控单元，分别是重置门和更新门；")]),t._v(" "),_("p",[t._v("重置门的作用是：在计算候选隐藏状态\\tilde{\\boldsymbol{H}}_{t}时，控制上一时间步隐藏状态哪些信息需要去遗忘；")]),t._v(" "),_("p",[t._v("更新门的作用是：更新当前时间步的隐藏状态时，组合上一时间步隐藏状态和当前时间步候选隐藏状态\\tilde{\\boldsymbol{H}}_{t}。")])]),t._v(" "),_("p",[t._v("详细过程见下述公式：")]),t._v("\n\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\\\\\n\\boldsymbol{H}_{t}=\\boldsymbol{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}\n\n"),_("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://s2.loli.net/2022/05/04/nOo136w7RJeaLdt.png",alt:"image-20220504092205544"}}),t._v(" "),_("h2",{attrs:{id:"_5-lstm-长短记忆神经网络"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_5-lstm-长短记忆神经网络"}},[t._v("#")]),t._v(" 5 LSTM-长短记忆神经网络")]),t._v(" "),_("p",[t._v("引入了3个门和记忆细胞，控制信息传递")]),t._v(" "),_("p",[t._v("输入门：哪些信息需要流入当前记忆细胞")]),t._v(" "),_("p",[t._v("输出门：哪些记忆信息流入隐藏状态")]),t._v(" "),_("p",[_("img",{attrs:{src:"https://s2.loli.net/2022/05/04/yua9jPmr25wn7xc.png",alt:"image-20220504094308427"}})]),t._v(" "),_("p",[t._v("simpleRNN：很容易遗忘")]),t._v(" "),_("p",[t._v("LSTM：可以记住比较长的信息")]),t._v(" "),_("p",[t._v("能用LSTM就别用RNN，效果一定优于simpleRNN")]),t._v(" "),_("p",[_("strong",[t._v("LSTM能够有更长记忆的原因")]),t._v("：在前向传播中，如果将输入的一串序列当做一部戏剧，那么LSTM的"),_("strong",[t._v("cell就是记录下的主线")]),t._v("，而遗忘门，输入门都用于给主线增加一些元素（比如新的角色，关键性的转机）。通过训练，"),_("strong",[t._v("遗忘门")]),t._v("能够针对性地对主线进行修改，选择“保留”或是“遗忘”过去主线中出现的内容，"),_("strong",[t._v("输入门")]),t._v("用于判断是否要输入新的内容，并且输入内容。"),_("strong",[t._v("输出门")]),t._v("则用于整合cell状态，判断需要把什么内容提取出来传递给下一层神经元。")]),t._v(" "),_("h2",{attrs:{id:"_6-stacked-lstm"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_6-stacked-lstm"}},[t._v("#")]),t._v(" 6 Stacked LSTM")]),t._v(" "),_("p",[t._v("多层LSTM：")]),t._v(" "),_("p",[t._v("多层累加")]),t._v(" "),_("h2",{attrs:{id:"_7-bidirectional-rnn"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_7-bidirectional-rnn"}},[t._v("#")]),t._v(" 7 Bidirectional RNN")]),t._v(" "),_("p",[t._v("双向RNN")]),t._v(" "),_("p",[t._v("从前到后，从后到前；同时训练；")]),t._v(" "),_("p",[t._v("能用双向RNN，就别用单向的，一定比单向的效果来得好")]),t._v(" "),_("p",[t._v("把两条LSTM的隐层输出，ht1和ht2做concatenation，作为最终的特征")]),t._v(" "),_("p",[_("strong",[t._v("Pretrain 预训练")]),t._v("（让神经网络有较好的初始化）")]),t._v(" "),_("p",[t._v("两个数据集或者任务越接近，之后的transfer迁移效果就会越好")]),t._v(" "),_("p",[t._v("提前训练embedding层，在大数据集上预训练embedding层")]),t._v(" "),_("p",[t._v("预训练就是，用大数据集去提前训练一个网络，然后保留一些我们想要的层（比如保留embedding层），然后用我们自己的小数据集去训练，记住：这个时候要保持embedding的参数和结构不变！")]),t._v(" "),_("p",[t._v("一般我们都对embedding层进行预训练，因为embedding层的参数一般都很大（vocabulary_size * embedding_dim ： 10000*300；如果词汇量有10000个，词向量维度设为300，那么embedding层的参数量就有300w个，很容易过拟合，所以我们采用大数据集先预训练！）")]),t._v(" "),_("h2",{attrs:{id:"_8-attention"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_8-attention"}},[t._v("#")]),t._v(" 8 Attention")]),t._v(" "),_("h2",{attrs:{id:"_9-self-attention"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_9-self-attention"}},[t._v("#")]),t._v(" 9 self-attention")]),t._v(" "),_("h2",{attrs:{id:"_10-transformer"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_10-transformer"}},[t._v("#")]),t._v(" 10 transformer")])])}),[],!1,null,null,null);s.default=e.exports}}]);