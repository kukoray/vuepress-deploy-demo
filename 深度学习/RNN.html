<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>RNN | JACKY</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/logo.png">
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.e4d8b86b.css" as="style"><link rel="preload" href="/assets/js/app.6d855320.js" as="script"><link rel="preload" href="/assets/js/2.d4ff5c60.js" as="script"><link rel="preload" href="/assets/js/58.6210b964.js" as="script"><link rel="prefetch" href="/assets/js/10.0677b883.js"><link rel="prefetch" href="/assets/js/100.47224151.js"><link rel="prefetch" href="/assets/js/101.e068f649.js"><link rel="prefetch" href="/assets/js/102.2216aa0a.js"><link rel="prefetch" href="/assets/js/103.1274fdb9.js"><link rel="prefetch" href="/assets/js/104.3894d45f.js"><link rel="prefetch" href="/assets/js/105.c45ae0dd.js"><link rel="prefetch" href="/assets/js/106.1ea76166.js"><link rel="prefetch" href="/assets/js/107.e9e69cdf.js"><link rel="prefetch" href="/assets/js/108.86dc789c.js"><link rel="prefetch" href="/assets/js/109.dff0ff70.js"><link rel="prefetch" href="/assets/js/11.0ce9576d.js"><link rel="prefetch" href="/assets/js/110.3a698495.js"><link rel="prefetch" href="/assets/js/111.8fc0aa53.js"><link rel="prefetch" href="/assets/js/112.aebd4f61.js"><link rel="prefetch" href="/assets/js/113.ac824a62.js"><link rel="prefetch" href="/assets/js/114.09b2f358.js"><link rel="prefetch" href="/assets/js/115.31a681fc.js"><link rel="prefetch" href="/assets/js/116.2eb333c2.js"><link rel="prefetch" href="/assets/js/117.45d3427d.js"><link rel="prefetch" href="/assets/js/118.313f7765.js"><link rel="prefetch" href="/assets/js/119.985d374f.js"><link rel="prefetch" href="/assets/js/12.809e25a2.js"><link rel="prefetch" href="/assets/js/120.41447dc4.js"><link rel="prefetch" href="/assets/js/121.f014baf8.js"><link rel="prefetch" href="/assets/js/122.097f6079.js"><link rel="prefetch" href="/assets/js/123.afc6a702.js"><link rel="prefetch" href="/assets/js/124.01d724f7.js"><link rel="prefetch" href="/assets/js/125.c9dc4beb.js"><link rel="prefetch" href="/assets/js/126.d5b795cb.js"><link rel="prefetch" href="/assets/js/127.ca74f94d.js"><link rel="prefetch" href="/assets/js/128.d2da9329.js"><link rel="prefetch" href="/assets/js/129.c8717ca5.js"><link rel="prefetch" href="/assets/js/13.152c7e44.js"><link rel="prefetch" href="/assets/js/14.1e51776e.js"><link rel="prefetch" href="/assets/js/15.baa617fb.js"><link rel="prefetch" href="/assets/js/16.f85fffe6.js"><link rel="prefetch" href="/assets/js/17.e6803744.js"><link rel="prefetch" href="/assets/js/18.6d74ca46.js"><link rel="prefetch" href="/assets/js/19.ab7532ab.js"><link rel="prefetch" href="/assets/js/20.be01556b.js"><link rel="prefetch" href="/assets/js/21.d9aded00.js"><link rel="prefetch" href="/assets/js/22.59fd96e2.js"><link rel="prefetch" href="/assets/js/23.575b0571.js"><link rel="prefetch" href="/assets/js/24.d7911b48.js"><link rel="prefetch" href="/assets/js/25.ee1ddc47.js"><link rel="prefetch" href="/assets/js/26.2645d0d2.js"><link rel="prefetch" href="/assets/js/27.b445999f.js"><link rel="prefetch" href="/assets/js/28.e93df91c.js"><link rel="prefetch" href="/assets/js/29.2e9e94e3.js"><link rel="prefetch" href="/assets/js/3.cce6fa70.js"><link rel="prefetch" href="/assets/js/30.c46c1576.js"><link rel="prefetch" href="/assets/js/31.d700821d.js"><link rel="prefetch" href="/assets/js/32.4017f172.js"><link rel="prefetch" href="/assets/js/33.c2f92eee.js"><link rel="prefetch" href="/assets/js/34.384d918e.js"><link rel="prefetch" href="/assets/js/35.9c122bb0.js"><link rel="prefetch" href="/assets/js/36.4ee62a31.js"><link rel="prefetch" href="/assets/js/37.cc538cd0.js"><link rel="prefetch" href="/assets/js/38.66fe6d97.js"><link rel="prefetch" href="/assets/js/39.62865922.js"><link rel="prefetch" href="/assets/js/4.ba8c4904.js"><link rel="prefetch" href="/assets/js/40.18d8b6ea.js"><link rel="prefetch" href="/assets/js/41.df0d8cab.js"><link rel="prefetch" href="/assets/js/42.2c76aec4.js"><link rel="prefetch" href="/assets/js/43.35b2ed84.js"><link rel="prefetch" href="/assets/js/44.7832dc93.js"><link rel="prefetch" href="/assets/js/45.bda8de55.js"><link rel="prefetch" href="/assets/js/46.6dfbc71d.js"><link rel="prefetch" href="/assets/js/47.ce3854ed.js"><link rel="prefetch" href="/assets/js/48.a0914b71.js"><link rel="prefetch" href="/assets/js/49.f409f02c.js"><link rel="prefetch" href="/assets/js/5.ebbfaeab.js"><link rel="prefetch" href="/assets/js/50.4f56073a.js"><link rel="prefetch" href="/assets/js/51.876c612d.js"><link rel="prefetch" href="/assets/js/52.d1cb790e.js"><link rel="prefetch" href="/assets/js/53.0e7bdacf.js"><link rel="prefetch" href="/assets/js/54.69554266.js"><link rel="prefetch" href="/assets/js/55.e91b1adb.js"><link rel="prefetch" href="/assets/js/56.629bc4a4.js"><link rel="prefetch" href="/assets/js/57.dd0e1f38.js"><link rel="prefetch" href="/assets/js/59.3171311f.js"><link rel="prefetch" href="/assets/js/6.319d6800.js"><link rel="prefetch" href="/assets/js/60.f28df51c.js"><link rel="prefetch" href="/assets/js/61.9d91b4cf.js"><link rel="prefetch" href="/assets/js/62.64dca654.js"><link rel="prefetch" href="/assets/js/63.2422c5f1.js"><link rel="prefetch" href="/assets/js/64.2a1e3d2b.js"><link rel="prefetch" href="/assets/js/65.f58499a8.js"><link rel="prefetch" href="/assets/js/66.c0efeb82.js"><link rel="prefetch" href="/assets/js/67.661d7923.js"><link rel="prefetch" href="/assets/js/68.dfab90f4.js"><link rel="prefetch" href="/assets/js/69.d9aed373.js"><link rel="prefetch" href="/assets/js/7.b1af566c.js"><link rel="prefetch" href="/assets/js/70.42a55bd7.js"><link rel="prefetch" href="/assets/js/71.26f59ddc.js"><link rel="prefetch" href="/assets/js/72.08c4a8c5.js"><link rel="prefetch" href="/assets/js/73.3f37d91a.js"><link rel="prefetch" href="/assets/js/74.e351145c.js"><link rel="prefetch" href="/assets/js/75.f4de590b.js"><link rel="prefetch" href="/assets/js/76.67c626c7.js"><link rel="prefetch" href="/assets/js/77.f1130886.js"><link rel="prefetch" href="/assets/js/78.778d639c.js"><link rel="prefetch" href="/assets/js/79.86f9f4d3.js"><link rel="prefetch" href="/assets/js/8.3b56d639.js"><link rel="prefetch" href="/assets/js/80.4ee16963.js"><link rel="prefetch" href="/assets/js/81.01badefa.js"><link rel="prefetch" href="/assets/js/82.4613395c.js"><link rel="prefetch" href="/assets/js/83.03b7e3d1.js"><link rel="prefetch" href="/assets/js/84.a8ed7238.js"><link rel="prefetch" href="/assets/js/85.d0a23cae.js"><link rel="prefetch" href="/assets/js/86.47c348c3.js"><link rel="prefetch" href="/assets/js/87.7103da76.js"><link rel="prefetch" href="/assets/js/88.03092892.js"><link rel="prefetch" href="/assets/js/89.9e2e01ba.js"><link rel="prefetch" href="/assets/js/9.1ad63360.js"><link rel="prefetch" href="/assets/js/90.bc109fb6.js"><link rel="prefetch" href="/assets/js/91.edd4ee6f.js"><link rel="prefetch" href="/assets/js/92.47eef26d.js"><link rel="prefetch" href="/assets/js/93.f3dd468d.js"><link rel="prefetch" href="/assets/js/94.5928d8ae.js"><link rel="prefetch" href="/assets/js/95.a7a96583.js"><link rel="prefetch" href="/assets/js/96.b8df9976.js"><link rel="prefetch" href="/assets/js/97.3200a463.js"><link rel="prefetch" href="/assets/js/98.29d61ae7.js"><link rel="prefetch" href="/assets/js/99.25ff9892.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e4d8b86b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">JACKY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/" aria-current="page" class="sidebar-link">/</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Linux操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>专业课</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/深度学习/CNN.html" class="sidebar-link">CNN卷积神经网络</a></li><li><a href="/深度学习/ImageNet.html" class="sidebar-link">ImageNet</a></li><li><a href="/深度学习/RNN.html" class="active sidebar-link">RNN</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_0-文本预处理" class="sidebar-link">0 文本预处理</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_1-序列数据" class="sidebar-link">1 序列数据</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_2-语言模型" class="sidebar-link">2 语言模型</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_3-循环神经网络" class="sidebar-link">3 循环神经网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_3-1-循环神经网络的反向传播" class="sidebar-link">3.1 循环神经网络的反向传播</a></li></ul></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_4-gru-门控循环单元" class="sidebar-link">4 GRU-门控循环单元</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_4-1-梯度裁剪" class="sidebar-link">4.1 梯度裁剪</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_4-2-重置门-更新门" class="sidebar-link">4.2 重置门&amp;&amp;更新门</a></li></ul></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_5-lstm-长短记忆神经网络" class="sidebar-link">5 LSTM-长短记忆神经网络</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_6-stacked-lstm" class="sidebar-link">6 Stacked LSTM</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_7-bidirectional-rnn" class="sidebar-link">7 Bidirectional RNN</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_8-attention" class="sidebar-link">8 Attention</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_9-self-attention" class="sidebar-link">9 self-attention</a></li><li class="sidebar-sub-header"><a href="/深度学习/RNN.html#_10-transformer" class="sidebar-link">10 transformer</a></li></ul></li><li><a href="/深度学习/SSD.html" class="sidebar-link">SSD目标检测</a></li><li><a href="/深度学习/manifold.html" class="sidebar-link">流形 Manifold</a></li><li><a href="/深度学习/优化器.html" class="sidebar-link">优化器</a></li><li><a href="/深度学习/归一化.html" class="sidebar-link">归一化方法</a></li><li><a href="/深度学习/数字图像处理.html" class="sidebar-link">数字图像处理</a></li><li><a href="/深度学习/欧拉增强的心率检测.html" class="sidebar-link">eulerian heartrate detection</a></li><li><a href="/深度学习/深度学习常见问题.html" class="sidebar-link">常见问题</a></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>论文</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>硬件设计</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法与数据结构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程语言</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>联邦学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件开发</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="rnn"><a href="#rnn" class="header-anchor">#</a> RNN</h1> <h2 id="_0-文本预处理"><a href="#_0-文本预处理" class="header-anchor">#</a> 0 文本预处理</h2> <img src="https://s2.loli.net/2023/02/26/qOmgnesoLvtkxXY.png" alt="image-20230226171802884" style="zoom:67%;"> <img src="https://s2.loli.net/2023/02/26/cqgvf4QGhosxUbJ.png" alt="image-20230226172026407" style="zoom:67%;"> <p><strong>step1：文本处理text to sequence</strong></p> <ol><li><p>tokenization（text to words），把一个文本分成一个个的单词</p></li> <li><p>count word frequency（统计词频）：保留常用词，去除低频词（名字、错误词），把每个单词映射成一个正整数</p></li> <li><p>one-hot encoding</p> <img src="https://s2.loli.net/2023/02/26/Y83BFGyMPVlnHNi.png" alt="image-20230226162358394" style="zoom:50%;"></li> <li><p>align sequence</p> <img src="https://s2.loli.net/2023/02/26/Ci6Yg5M2cxPZpk3.png" alt="image-20230226162630587" style="zoom:50%;"></li></ol> <p><strong>step2：word embedding（word to vector）</strong></p> <p>在step1中的 align sequence之后（假设都对其到 input_length  = 20 ），那么每个样例的输入长度都是20个words；</p> <p>但是我们需要对这words转为vector，可以直接想到的是用one-hot方法，对于我们得到的vocabulary（词汇量，字典，一般很大，比如10k），那么使用one-hot编码，那么得到的vector也会很大，一个单词就是10k维度。</p> <p>所以我们采用word embedding（他的思路就是，把one-hot向量给降维，或者说是做了一个线性变换，就是乘上一个矩阵）</p> <img src="https://s2.loli.net/2023/02/26/K4rUqtBhFTgInO3.png" alt="image-20230226170434127" style="zoom:50%;"> <p>这里的d是最后我们对于每个单词表示的词向量的维度（embedding_dim），v是vocabulary的词汇量大小</p> <p>embedding层的参数 =  vocabulary * embedding_dim</p> <h2 id="_1-序列数据"><a href="#_1-序列数据" class="header-anchor">#</a> 1 序列数据</h2> <p>序列数据：前后数据通常具有<strong>关联性</strong></p> <h2 id="_2-语言模型"><a href="#_2-语言模型" class="header-anchor">#</a> 2 语言模型</h2> <p>NLP</p> <p>时间步：一个单词算一个time step；</p> <p>联合概率：</p> <img src="https://s2.loli.net/2022/05/02/LEol7YsvWt1mQzk.png" alt="image-20220502111757867" style="zoom:50%;"> <p>以上的概率，由语料库中的词频统计来得到。</p> <p>缺点：第i个词，依赖第i-1个词。随着时间步的增大，计算量呈指数递增。</p> <h2 id="_3-循环神经网络"><a href="#_3-循环神经网络" class="header-anchor">#</a> 3 循环神经网络</h2> <p>​		RNN是针对序列数据而生的神经网络结构，核心在于循环使用网络层参数，避免时间步增大带来的参数激增，并引入<strong>隐藏状态</strong>，用于记录历史信息，有效的处理数据的前后关联性。</p> <p><img src="https://s2.loli.net/2023/02/26/9Xn6FMSdtZ5W2mN.png" alt="image-20230226172817676"></p> <p><img src="https://s2.loli.net/2022/05/03/ZWqQodMPRl4D2XE.png" alt="image-20220503114850772"></p> <p>三个权重\boldsymbol{W}_{x h}、\boldsymbol{W}_{h h}、\boldsymbol{W}_{h q}，会循环使用。</p>
\begin{array}{l}
\boldsymbol{H}_{t}=\phi\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right) \\
\boldsymbol{O}_{t}=\boldsymbol{H}_{t} \boldsymbol{W}_{h q}+\boldsymbol{b}_{q}
\end{array}

<p>这里\boldsymbol{X}_{t}是在时间步为t时的<strong>输入数据</strong>，而\boldsymbol{O}_{t}是时间步为t时的<strong>输出数据</strong>；其中的\boldsymbol{H}_{t}为时间步为t的<strong>隐藏状态</strong>（用于记录历史信息）</p> <p>其中<code>隐藏状态</code>的作用是：用于记录历史信息，有效处理数据的前后关联性。激活函数采用==Tanh==（也就是上述公式中的$ \phi $），将输出值域控制在（-1,1），防止数值呈指数级变化。</p> <p>RNN特性：</p> <ol><li>循环神经网络的==隐藏状态==可以捕捉截至当前时间步的序列的历史信息</li> <li>循环神经网络模型参数的数量不随时间步的增加而增长</li></ol> <p>问题：</p> <ul><li><p>为什么simple RNN 使用tanh函数</p> <p>由于RNN 的ht的计算依赖于上一次的ht-1，可以简单理解为是一个累乘的过程，所以对于参数矩阵A中的参数，很容易出现参数累乘后爆炸、或者参数累乘后消失的情况。通过tanh函数，就可以把每一次得到的新ht的参数范围控制在-1~1之间，从而避免ht中参数过大或者消失的情况。</p></li> <li><p>Simple RNN的缺点</p> <p>simple RNN只能记住短序列的，对于长序列的输入，后面的隐藏状态会遗忘前面的输入。LSTM则比simple RNN的记忆时间长很多</p></li></ul> <h3 id="_3-1-循环神经网络的反向传播"><a href="#_3-1-循环神经网络的反向传播" class="header-anchor">#</a> 3.1 循环神经网络的反向传播</h3> <p>穿越时间的反向传播</p> <p>致命<strong>缺点</strong>：梯度随时间t呈指数变化（主要是看参数\boldsymbol{W}_{hh}），容易引发<code>梯度消失</code>、<code>梯度爆炸</code></p> <h2 id="_4-gru-门控循环单元"><a href="#_4-gru-门控循环单元" class="header-anchor">#</a> 4 GRU-门控循环单元</h2> <p>引入了<strong>门</strong>的循环神经网络——GRU</p> <p>对于梯度爆炸，我们可以采用<strong>梯度裁剪</strong>的方式去解决；但是对于梯度消失，我们无法解决。</p> <h3 id="_4-1-梯度裁剪"><a href="#_4-1-梯度裁剪" class="header-anchor">#</a> 4.1 梯度裁剪</h3> <p>梯度裁剪是解决梯度爆炸的一种技术，其出发点是非常简明的：如果梯度变得非常大，那么我们就调节它使其保持较小的状态。精确的说，如果∥ g ∥ ≥ c ，则</p>
g ← c ⋅ g / ∥ g ∥

<p>​		此处的c指超参数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span>指梯度，∥ g ∥为梯度的<a href="https://so.csdn.net/so/search?q=%E8%8C%83%E6%95%B0&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">范数<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，g / ∥ g ∥必然是个单位矢量，因此在进行调节后新的梯度范数必然等于c，注意到如果∥ g ∥ ≤ c则不需要进行调节。
  梯度裁剪确保了梯度矢量的最大范数（本文中规定为c）。即使在模型的损失函数不规则时，这一技巧也有助于梯度下降保持合理的行为。下面的图片展示了损失函数的陡崖。不采用裁剪，参数将会沿着梯度下降方向剧烈变化，导致其离开了最小值范围；而使用裁剪后参数变化将被限制在一个合理范围内，避免了上面的情况。</p> <img src="https://s2.loli.net/2022/05/04/phYExNtbcVFdOoz.png" alt="在这里插入图片描述" style="zoom:50%;"> <h3 id="_4-2-重置门-更新门"><a href="#_4-2-重置门-更新门" class="header-anchor">#</a> 4.2 重置门&amp;&amp;更新门</h3> <p>作用：缓解RNN<strong>梯度消失</strong>带来的问题，引入门的概念，来控制信息流动，使模型更好的记住长远时期的信息，并缓解梯度消失。</p> <p>重置门：哪些信息需要遗忘</p> <p>更新门：哪些信息需要注意</p> <img src="https://s2.loli.net/2022/05/04/E65tH8kD1TGeu3j.png" alt="image-20220504090846784" style="zoom:53%;">
$$
\begin{array}{l}
重置门：\boldsymbol{R}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x r}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h r}+\boldsymbol{b}_{r}\right) \\
更新门：\boldsymbol{Z}_{t}=\sigma\left(\boldsymbol{X}_{t} \boldsymbol{W}_{x z}+\boldsymbol{H}_{t-1} \boldsymbol{W}_{h z}+\boldsymbol{b}_{z}\right)
\end{array}
$$
<p>这里的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span>是激活函数，此处采用的是sigmoid激活函数，使门的值为（0,1），0表示遗忘，1表示保留。</p> <p>GRU和传统RNN的区别：</p> <blockquote><p>GRU引入了门控单元，分别是重置门和更新门；</p> <p>重置门的作用是：在计算候选隐藏状态\tilde{\boldsymbol{H}}_{t}时，控制上一时间步隐藏状态哪些信息需要去遗忘；</p> <p>更新门的作用是：更新当前时间步的隐藏状态时，组合上一时间步隐藏状态和当前时间步候选隐藏状态\tilde{\boldsymbol{H}}_{t}。</p></blockquote> <p>详细过程见下述公式：</p>
\tilde{\boldsymbol{H}}_{t}=\tanh \left(\boldsymbol{X}_{t} \boldsymbol{W}_{x h}+\left(\boldsymbol{R}_{t} \odot \boldsymbol{H}_{t-1}\right) \boldsymbol{W}_{h h}+\boldsymbol{b}_{h}\right)\\
\boldsymbol{H}_{t}=\boldsymbol{Z}_{t} \odot \boldsymbol{H}_{t-1}+\left(1-\boldsymbol{Z}_{t}\right) \odot \tilde{\boldsymbol{H}}_{t}

<img src="https://s2.loli.net/2022/05/04/nOo136w7RJeaLdt.png" alt="image-20220504092205544" style="zoom:50%;"> <h2 id="_5-lstm-长短记忆神经网络"><a href="#_5-lstm-长短记忆神经网络" class="header-anchor">#</a> 5 LSTM-长短记忆神经网络</h2> <p>引入了3个门和记忆细胞，控制信息传递</p> <p>输入门：哪些信息需要流入当前记忆细胞</p> <p>输出门：哪些记忆信息流入隐藏状态</p> <p><img src="https://s2.loli.net/2022/05/04/yua9jPmr25wn7xc.png" alt="image-20220504094308427"></p> <p>simpleRNN：很容易遗忘</p> <p>LSTM：可以记住比较长的信息</p> <p>能用LSTM就别用RNN，效果一定优于simpleRNN</p> <p><strong>LSTM能够有更长记忆的原因</strong>：在前向传播中，如果将输入的一串序列当做一部戏剧，那么LSTM的<strong>cell就是记录下的主线</strong>，而遗忘门，输入门都用于给主线增加一些元素（比如新的角色，关键性的转机）。通过训练，<strong>遗忘门</strong>能够针对性地对主线进行修改，选择“保留”或是“遗忘”过去主线中出现的内容，<strong>输入门</strong>用于判断是否要输入新的内容，并且输入内容。<strong>输出门</strong>则用于整合cell状态，判断需要把什么内容提取出来传递给下一层神经元。</p> <h2 id="_6-stacked-lstm"><a href="#_6-stacked-lstm" class="header-anchor">#</a> 6 Stacked LSTM</h2> <p>多层LSTM：</p> <p>多层累加</p> <h2 id="_7-bidirectional-rnn"><a href="#_7-bidirectional-rnn" class="header-anchor">#</a> 7 Bidirectional RNN</h2> <p>双向RNN</p> <p>从前到后，从后到前；同时训练；</p> <p>能用双向RNN，就别用单向的，一定比单向的效果来得好</p> <p>把两条LSTM的隐层输出，ht1和ht2做concatenation，作为最终的特征</p> <p><strong>Pretrain 预训练</strong>（让神经网络有较好的初始化）</p> <p>两个数据集或者任务越接近，之后的transfer迁移效果就会越好</p> <p>提前训练embedding层，在大数据集上预训练embedding层</p> <p>预训练就是，用大数据集去提前训练一个网络，然后保留一些我们想要的层（比如保留embedding层），然后用我们自己的小数据集去训练，记住：这个时候要保持embedding的参数和结构不变！</p> <p>一般我们都对embedding层进行预训练，因为embedding层的参数一般都很大（vocabulary_size * embedding_dim ： 10000*300；如果词汇量有10000个，词向量维度设为300，那么embedding层的参数量就有300w个，很容易过拟合，所以我们采用大数据集先预训练！）</p> <h2 id="_8-attention"><a href="#_8-attention" class="header-anchor">#</a> 8 Attention</h2> <h2 id="_9-self-attention"><a href="#_9-self-attention" class="header-anchor">#</a> 9 self-attention</h2> <h2 id="_10-transformer"><a href="#_10-transformer" class="header-anchor">#</a> 10 transformer</h2></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">最后更新时间:</span> <span class="time">1 分钟前</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/深度学习/ImageNet.html" class="prev">
        ImageNet
      </a></span> <span class="next"><a href="/深度学习/SSD.html">
        SSD目标检测
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.6d855320.js" defer></script><script src="/assets/js/2.d4ff5c60.js" defer></script><script src="/assets/js/58.6210b964.js" defer></script>
  </body>
</html>
