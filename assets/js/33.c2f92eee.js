(window.webpackJsonp=window.webpackJsonp||[]).push([[33],{451:function(v,_,l){"use strict";l.r(_);var i=l(65),t=Object(i.a)({},(function(){var v=this,_=v.$createElement,l=v._self._c||_;return l("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[l("h1",{attrs:{id:"前馈神经网络"}},[l("a",{staticClass:"header-anchor",attrs:{href:"#前馈神经网络"}},[v._v("#")]),v._v(" 前馈神经网络")]),v._v(" "),l("p",[v._v("机器学习我们已经知道可以分为两大流派：")]),v._v(" "),l("ol",[l("li",[l("p",[v._v("频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：")]),v._v(" "),l("ol",[l("li",[l("p",[v._v("正则化，L1，L2 等")])]),v._v(" "),l("li",[l("p",[v._v("核化，如核支撑向量机")])]),v._v(" "),l("li",[l("p",[v._v("集成化，AdaBoost，RandomForest")])]),v._v(" "),l("li",[l("p",[v._v("层次化，神经网络，神经网络有各种不同的模型，有代表性的有：")]),v._v(" "),l("ol",[l("li",[v._v("多层感知机")]),v._v(" "),l("li",[v._v("Autoencoder")]),v._v(" "),l("li",[v._v("CNN")]),v._v(" "),l("li",[v._v("RNN")])]),v._v(" "),l("p",[v._v("这几种模型又叫做深度神经网络。")])])])]),v._v(" "),l("li",[l("p",[v._v("贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：")]),v._v(" "),l("ol",[l("li",[v._v("有向图-贝叶斯网络，加入层次化后有深度有向网络，包括\n"),l("ol",[l("li",[v._v("Sigmoid Belief Network")]),v._v(" "),l("li",[v._v("Variational Autoencoder")]),v._v(" "),l("li",[v._v("GAN")])])]),v._v(" "),l("li",[v._v("无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。")]),v._v(" "),l("li",[v._v("混合，加入层次化后有深度信念网络")])]),v._v(" "),l("p",[v._v("这几个加入层次化后的模型叫做深度生成网络。")])])]),v._v(" "),l("p",[v._v("从广义来说，深度学习包括深度生成网络和深度神经网络。")]),v._v(" "),l("h2",{attrs:{id:"from-pla-to-dl"}},[l("a",{staticClass:"header-anchor",attrs:{href:"#from-pla-to-dl"}},[v._v("#")]),v._v(" From PLA to DL")]),v._v(" "),l("ul",[l("li",[v._v("1958，PLA")]),v._v(" "),l("li",[v._v("1969，PLA 不能解决 XOR 等非线性数据")]),v._v(" "),l("li",[v._v("1981，MLP，多层感知机的出现解决了上面的问题")]),v._v(" "),l("li",[v._v("1986，BP 算法应用在 MLP 上，RNN")]),v._v(" "),l("li",[v._v("1989，CNN，Univeral Approximation Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP 算法的梯度消失问题")]),v._v(" "),l("li",[v._v("1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL 逐渐没落")]),v._v(" "),l("li",[v._v("1997，LSTM")]),v._v(" "),l("li",[v._v("2006，基于 RBM 的 深度信念网络和深度自编码")]),v._v(" "),l("li",[v._v("2009，GPU的发展")]),v._v(" "),l("li",[v._v("2011，在语音方面的应用")]),v._v(" "),l("li",[v._v("2012，ImageNet")]),v._v(" "),l("li",[v._v("2013，VAE")]),v._v(" "),l("li",[v._v("2014，GAN")]),v._v(" "),l("li",[v._v("2016，AlphaGo")]),v._v(" "),l("li",[v._v("2018，GNN")])]),v._v(" "),l("p",[v._v("DL 不是一个新的东西，其近年来的大发展主要原因如下：")]),v._v(" "),l("ol",[l("li",[v._v("数据量变大")]),v._v(" "),l("li",[v._v("分布式计算的发展")]),v._v(" "),l("li",[v._v("硬件算力的发展")])]),v._v(" "),l("h2",{attrs:{id:"非线性问题"}},[l("a",{staticClass:"header-anchor",attrs:{href:"#非线性问题"}},[v._v("#")]),v._v(" 非线性问题")]),v._v(" "),l("p",[v._v("对于非线性的问题，有三种方法：")]),v._v(" "),l("ol",[l("li",[v._v("非线性转换，将低维空间转换到高维空间（Cover 定理），从而变为一个线性问题。")]),v._v(" "),l("li",[v._v("核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。")]),v._v(" "),l("li",[v._v("神经网络方法，将复合运算变为基本的线性运算的组合。")])])])}),[],!1,null,null,null);_.default=t.exports}}]);