(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{481:function(t,a,r){"use strict";r.r(a);var s=r(65),e=Object(s.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"优化器"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#优化器"}},[t._v("#")]),t._v(" 优化器")]),t._v(" "),r("p",[t._v("机器学习中，通常有很多方法来试图寻找模型的最优解。比如常见的")]),t._v(" "),r("p",[r("strong",[t._v("梯度下降法(Gradient Descent)")])]),t._v(" "),r("blockquote",[r("p",[t._v("随机梯度下降法SGD")]),t._v(" "),r("p",[t._v("小批量梯度下降MSGD")]),t._v(" "),r("p",[t._v("批量梯度下降法BGD")])]),t._v(" "),r("p",[r("strong",[t._v("动量优化法(Momentum)")])]),t._v(" "),r("p",[r("strong",[t._v("自适应学习率优化算法")])]),t._v(" "),r("blockquote",[r("p",[t._v("AdaGrad算法\nRMSProp算法\nAdam算法（吸收了adaGrad和RMSProp的优点）\nlazyadam算法")])]),t._v(" "),r("h2",{attrs:{id:"sgd随机梯度下降"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#sgd随机梯度下降"}},[t._v("#")]),t._v(" SGD随机梯度下降")]),t._v(" "),r("p",[t._v("由于梯度下降，每次迭代需要用到所有的数据，耗时很长，计算量很大。")])])}),[],!1,null,null,null);a.default=e.exports}}]);